{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 02\n",
    "\n",
    "Content:\n",
    "\n",
    "1. Language modelling using bigrams\n",
    "2. Necessity of end of sentence token `<\\s>`\n",
    "3. Doing k-smoothing, we are still dealing with a probability\n",
    "4. Makemore Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Language modelling using bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/stephan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "emma = nltk.corpus.gutenberg.raw(fileids='austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.  Between _them_ it was more the intimacy\n",
      "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness o\n"
     ]
    }
   ],
   "source": [
    "print(emma[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[emma by jane austen 1816]\\n\\nvolume i\\n\\nchapter i\\n\\n\\nemma woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.',\n",
       " \"she was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.\",\n",
       " 'her mother\\nhad died too long ago for her to have more than an indistinct\\nremembrance of her caresses; and her place had been supplied\\nby an excellent woman as governess, who had fallen little short\\nof a mother in affection.',\n",
       " \"sixteen years had miss taylor been in mr. woodhouse's family,\\nless as a governess than a friend, very fond of both daughters,\\nbut particularly of emma.\",\n",
       " 'between _them_ it was more the intimacy\\nof sisters.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer, word_tokenize, sent_tokenize\n",
    "sentences = sent_tokenize(emma.lower())\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "- count all bigrams in sentences above (add a start and end of sentence token), since we want to generate new sentences\n",
    "- construct probabilities from the bigram counter above\n",
    "- to generate new sentences you start with `<s>` and generate a new word from the bigram probabilities until you reach the end of sentence token `<\\s>`\n",
    "- finally calculate the perplexity of your generated sentences and do the same for original sentences from `sentences`\n",
    "\n",
    "\n",
    "Tipps:\n",
    "- `nltk.utils` has a `ngrams` function\n",
    "- you might want to use `from collections import Counter`\n",
    "- `np.random.choice(candidate_tokens, size=1, p=candidate_probs)` can be used to draw tokens given computed probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Necessity of `<\\s>`\n",
    "\n",
    "Exercise 3.5 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/). Given a trainings corpus without `<\\s>`\n",
    "\n",
    "`<s> a b` <br>\n",
    "`<s> b a` <br>\n",
    "`<s> a a` <br>\n",
    "`<s> b b` <br>\n",
    "\n",
    "1. Use a bigram model and calculate the probability of all possible sentences with two words $\\{a, b\\}$. Show that the sum of all these probabilities add up to 1.\n",
    "2. Do the same for all possible sentences of lengths 3 of the words $\\{a, b\\}$. Show that these sum up to 1 as well.\n",
    "\n",
    "The point is, that this property is not very useful: we rather have a language model, that is able to produce a sentence of arbitrary length. When you are generating a sentence, you are not going to decide before hand, how many words you are going to use..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "First let's write down the formula for bigrams:\n",
    "\n",
    "$$\n",
    "\\mathbb P [ <\\! s \\! >, w_1, \\dots, w_n] = \\mathbb P [w_1 \\mid <\\!s \\! > ] \\mathbb P[w_2 \\mid w_1] \\cdots \\mathbb P[w_n \\mid w_{n-1}]\n",
    "$$\n",
    "\n",
    "here $n=2$ for part 1 and $n=3$ for part 2.\n",
    "\n",
    "Let's write $\\mathbb P[b \\mid a] = p_{b|a}$ and use $s$ instead of `<s>`, then from the training corpus we get:\n",
    "\n",
    "\n",
    "- $p_{a|s} = p_{b|s} = \\dots$\n",
    "- $\\dots$\n",
    "\n",
    "Recall that we are counting the number of occurrences, e.g.: $p_{b|a}= \\frac{C(a,b)}{C(a)} = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 2\n",
    "\n",
    "We must calculate all probabilities of sentences of lengths 3:\n",
    "\n",
    "- How many are there?\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. k-smoothing is still a probability\n",
    "\n",
    "Given a vocabulary $V$ and a bigram language model, applying k-smoothing (or additive smoothing), i.e.\n",
    "\n",
    "$$\\mathbb{P}[w_n \\mid w_{n-1}] = \\frac{C(w_n, w_{n-1}) + k}{ C(w_{n-1}) + k|V|}$$\n",
    "\n",
    "we still have a probability.\n",
    "\n",
    "We have to show, that when summing over all possible $w_n$, the sum still adds up to $1$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\sum_{w_n \\in V} \\mathbb P [w_n \\mid w_{n-1}] \n",
    "        &=\\sum_{w_n \\in V} \\frac{C(w_{n-1}, w_n) + k}{ C(w_{n-1}) + k|V|} \\\\\n",
    "        &= \\dots = 1\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Makemore Part I\n",
    "\n",
    "Watch Part I of [makemore](https://youtu.be/PaCmpygFfXo?si=7PonsCOBoNCcmWHo), until 1:02:56, and code along. We will come back to this video series in later weeks.\n",
    "\n",
    "- Write a function, that takes a prefix and completes this prefix according to the the implemented bigram-model (e.g. given the prefix `ann` possible completions could be `anna`, `anner`, `ann...`) \n",
    "- Loglikelihood:\n",
    "    - what happens to the loglikelihood if we use large smoothing factor $k$? \n",
    "    - Calculate the loglikelihood for all words for different $k$ values.\n",
    "    - make an appropriate plot with negative loglikelihood on the y-axis and logarithmic $k$ on the x-axis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
