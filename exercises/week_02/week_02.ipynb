{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 02\n",
    "\n",
    "Content:\n",
    "\n",
    "1. Language modelling using bigrams\n",
    "2. Necessity of end of sentence token `<\\s>`\n",
    "3. Doing k-smoothing, we are still dealing with a probability\n",
    "4. Makemore Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Language modelling using bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T17:15:38.459879Z",
     "start_time": "2025-03-02T17:15:34.967754Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/gideon/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "emma = nltk.corpus.gutenberg.raw(fileids='austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T17:15:38.465808Z",
     "start_time": "2025-03-02T17:15:38.462374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.  Between _them_ it was more the intimacy\n",
      "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness o\n"
     ]
    }
   ],
   "source": [
    "print(emma[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T17:15:38.680142Z",
     "start_time": "2025-03-02T17:15:38.467825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['[emma by jane austen 1816]\\n\\nvolume i\\n\\nchapter i\\n\\n\\nemma woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.',\n \"she was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.\",\n 'her mother\\nhad died too long ago for her to have more than an indistinct\\nremembrance of her caresses; and her place had been supplied\\nby an excellent woman as governess, who had fallen little short\\nof a mother in affection.',\n \"sixteen years had miss taylor been in mr. woodhouse's family,\\nless as a governess than a friend, very fond of both daughters,\\nbut particularly of emma.\",\n 'between _them_ it was more the intimacy\\nof sisters.']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer, word_tokenize, sent_tokenize\n",
    "sentences = sent_tokenize(emma.lower())\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "- count all bigrams in sentences above (add a start and end of sentence token), since we want to generate new sentences\n",
    "- construct probabilities from the bigram counter above\n",
    "- to generate new sentences you start with `<s>` and generate a new word from the bigram probabilities until you reach the end of sentence token `<\\s>`\n",
    "- finally calculate the perplexity of your generated sentences and do the same for original sentences from `sentences`\n",
    "\n",
    "\n",
    "Tipps:\n",
    "- `nltk.utils` has a `ngrams` function\n",
    "- you might want to use `from collections import Counter`\n",
    "- `np.random.choice(candidate_tokens, size=1, p=candidate_probs)` can be used to draw tokens given computed probabilities"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generierter Satz: \"that will be applied herself well be a persuasion.\n",
      "\n",
      "Perplexity des generierten Satzes: 42.989408329366725\n",
      "\n",
      "\n",
      "             Index   Perplexity\n",
      "count  7489.000000  7489.000000\n",
      "mean   3745.000000    37.774621\n",
      "std    2162.032416    14.338568\n",
      "min       1.000000     6.340000\n",
      "25%    1873.000000    29.080000\n",
      "50%    3745.000000    36.000000\n",
      "75%    5617.000000    43.710000\n",
      "max    7489.000000   181.420000\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "start_token = \"<s>\"\n",
    "end_token = \"<\\\\s>\"\n",
    "\n",
    "tokenized_sentences = []\n",
    "for sent in sentences:\n",
    "    tokens = [start_token] + sent.split() + [end_token]\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "bigram_counter = Counter()\n",
    "for tokens in tokenized_sentences:\n",
    "    for bigram in list(ngrams(tokens, 2)):\n",
    "        bigram_counter[bigram] += 1\n",
    "\n",
    "first_token_counts = Counter()\n",
    "for (w1, w2), count in bigram_counter.items():\n",
    "    first_token_counts[w1] += count\n",
    "\n",
    "bigram_prob = {}\n",
    "for (w1, w2), count in bigram_counter.items():\n",
    "    prob = count / first_token_counts[w1]\n",
    "    bigram_prob[(w1, w2)] = prob\n",
    "\n",
    "def generate_sentence():\n",
    "    current_token = start_token\n",
    "    sentence = []\n",
    "    while True:\n",
    "        candidates = []\n",
    "        candidate_probs = []\n",
    "        for (w1, w2), prob in bigram_prob.items():\n",
    "            if w1 == current_token:\n",
    "                candidates.append(w2)\n",
    "                candidate_probs.append(prob)\n",
    "                \n",
    "        if not candidates:\n",
    "            break\n",
    "            \n",
    "        candidate_probs = np.array(candidate_probs)\n",
    "        candidate_probs = candidate_probs / candidate_probs.sum()\n",
    "        next_token = np.random.choice(candidates, size=1, p=candidate_probs)[0]\n",
    "        \n",
    "        if next_token == end_token:\n",
    "            break\n",
    "        sentence.append(next_token)\n",
    "        current_token = next_token\n",
    "        \n",
    "    return \" \".join(sentence)\n",
    "\n",
    "generated_sentence = generate_sentence()\n",
    "print(\"Generierter Satz:\", generated_sentence)\n",
    "\n",
    "def sentence_perplexity(sentence):\n",
    "    tokens = [start_token] + sentence.split() + [end_token]\n",
    "    N = len(tokens) - 1 \n",
    "    log_prob_sum = 0.0\n",
    "    for i in range(N):\n",
    "        bigram = (tokens[i], tokens[i+1])\n",
    "        prob = bigram_prob.get(bigram, 1e-8)\n",
    "        log_prob_sum += math.log(prob)\n",
    "        \n",
    "    perplexity = math.exp(-log_prob_sum / N)\n",
    "    return perplexity\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for idx, sent in enumerate(sentences):\n",
    "    perplex = sentence_perplexity(sent)\n",
    "    data.append({\"Index\": idx + 1, \"Satz\": sent, \"Perplexity\": round(perplex, 2)})\n",
    "\n",
    "gen_perplexity = sentence_perplexity(generated_sentence)\n",
    "print(\"\\nPerplexity des generierten Satzes:\", gen_perplexity)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"\\n\")\n",
    "print(df.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T17:15:39.604002Z",
     "start_time": "2025-03-02T17:15:38.681538Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Necessity of `<\\s>`\n",
    "\n",
    "Exercise 3.5 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/). Given a trainings corpus without `<\\s>`\n",
    "\n",
    "`<s> a b` <br>\n",
    "`<s> b a` <br>\n",
    "`<s> a a` <br>\n",
    "`<s> b b` <br>\n",
    "\n",
    "1. Use a bigram model and calculate the probability of all possible sentences with two words $\\{a, b\\}$. Show that the sum of all these probabilities add up to 1.\n",
    "2. Do the same for all possible sentences of lengths 3 of the words $\\{a, b\\}$. Show that these sum up to 1 as well.\n",
    "\n",
    "The point is, that this property is not very useful: we rather have a language model, that is able to produce a sentence of arbitrary length. When you are generating a sentence, you are not going to decide before hand, how many words you are going to use..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "First let's write down the formula for bigrams:\n",
    "\n",
    "$$\n",
    "\\mathbb P [ <\\! s \\! >, w_1, \\dots, w_n] = \\mathbb P [w_1 \\mid <\\!s \\! > ] \\mathbb P[w_2 \\mid w_1] \\cdots \\mathbb P[w_n \\mid w_{n-1}]\n",
    "$$\n",
    "\n",
    "here $n=2$ for part 1 and $n=3$ for part 2.\n",
    "\n",
    "Let's write $\\mathbb P[b \\mid a] = p_{b|a}$ and use $s$ instead of `<s>`, then from the training corpus we get:\n",
    "\n",
    "\n",
    "- $p_{a|s} = p_{b|s} = \\dots$\n",
    "- $\\dots$\n",
    "\n",
    "Recall that we are counting the number of occurrences, e.g.: $p_{b|a}= \\frac{C(a,b)}{C(a)} = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bigram Wahrscheinlichkeiten:\n",
    "- $C(a) = 4$\n",
    "- $C(b) = 4$\n",
    " \n",
    "- $C(s, a) = 2$\n",
    "- $C(s, b) = 2$\n",
    "- $C(a, a) = 1$\n",
    "- $C(a, b) = 1$\n",
    "- $C(b, a) = 1$\n",
    "- $C(b, b) = 1$\n",
    "\n",
    "- $p_{a|s} = p_{b|s} = \\frac{C(s, a)}{C(a)} = \\frac{2}{4} = \\frac{1}{2}$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 2\n",
    "\n",
    "We must calculate all probabilities of sentences of lengths 3:\n",
    "\n",
    "- How many are there?\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "$|V| = 2$\n",
    "\n",
    "daraus ergibt sich das es $2^3 = 8$ mögliche Sätze gibt:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. k-smoothing is still a probability\n",
    "\n",
    "Given a vocabulary $V$ and a bigram language model, applying k-smoothing (or additive smoothing), i.e.\n",
    "\n",
    "$$\\mathbb{P}[w_n \\mid w_{n-1}] = \\frac{C(w_n, w_{n-1}) + k}{ C(w_{n-1}) + k|V|}$$\n",
    "\n",
    "we still have a probability.\n",
    "\n",
    "We have to show, that when summing over all possible $w_n$, the sum still adds up to $1$:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\sum_{w_n \\in V} \\mathbb P [w_n \\mid w_{n-1}] \n",
    "        &=\\sum_{w_n \\in V} \\frac{C(w_{n-1}, w_n) + k}{ C(w_{n-1}) + k|V|} \\\\\n",
    "        &=  \\frac{1}{C(w_{n-1}) + k|V|} \\sum_{w_n \\in V} (C(w_{n-1}, w_n) + k) \\\\ \n",
    "        &=  \\frac{1}{C(w_{n-1}) + k|V|} (\\sum_{w_n \\in V} C(w_{n-1}, w_n) + \\sum_{w_n \\in V} k) \\\\ \n",
    "        &=  \\frac{1}{C(w_{n-1}) + k|V|} (C(w_{n-1}) + k|V|) \\\\ \n",
    "        &=  \\frac{C(w_{n-1}) + k|V|}{C(w_{n-1}) + k|V|} \\\\ \n",
    "        = 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Zusätzliche Erklärung:\n",
    "\n",
    "Die Summe aller auf $w_{n-1}$ folgenden Wörter ergibt grad die Anzahl der vorkommnisse von $w_{n-1}$ also $C(w_{n-1})$.\n",
    "$\\sum_{w_n \\in V} C(w_{n-1}, w_n) = C(w_{n-1})$\n",
    "\n",
    "Die Summe von $k$ über alle Wörter ergibt $k|V|$, da es $|V|$ Wörter gibt.\n",
    "$ \\sum_{w_n \\in V} k = k|V|$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Makemore Part I\n",
    "\n",
    "Watch Part I of [makemore](https://youtu.be/PaCmpygFfXo?si=7PonsCOBoNCcmWHo), until 1:02:56, and code along. We will come back to this video series in later weeks.\n",
    "\n",
    "- Write a function, that takes a prefix and completes this prefix according to the the implemented bigram-model (e.g. given the prefix `ann` possible completions could be `anna`, `anner`, `ann...`) \n",
    "- Loglikelihood:\n",
    "    - what happens to the loglikelihood if we use large smoothing factor $k$? \n",
    "    - Calculate the loglikelihood for all words for different $k$ values.\n",
    "    - make an appropriate plot with negative loglikelihood on the y-axis and logarithmic $k$ on the x-axis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
