{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 04\n",
    "\n",
    "Content:\n",
    "\n",
    "1. Word Embeddings\n",
    "2. Word Analogy\n",
    "3. Makemore \n",
    "4. Sentiment Analysis using Word Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word Embeddings\n",
    "\n",
    "In this exercise, we are investigating word embeddings. More precisely we work with *word2vec* and *glove*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download pretrained models:\n",
    "- *word2vec on goolge news 300*: 300 dimensional embeddings trained on 3 billion words, [word2vec](https://code.google.com/archive/p/word2vec/)\n",
    "- *glove wiki gigaword 100*: 100 dimensional embedding trained on 6 billion wiki tokens, [glove](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/miniforge3/envs/py-nlp-exercises/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = api.load('word2vec-google-news-300')\n",
    "glove = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23088  ,  0.28283  ,  0.6318   , -0.59411  , -0.58599  ,\n",
       "        0.63255  ,  0.24402  , -0.14108  ,  0.060815 , -0.7898   ,\n",
       "       -0.29102  ,  0.14287  ,  0.72274  ,  0.20428  ,  0.1407   ,\n",
       "        0.98757  ,  0.52533  ,  0.097456 ,  0.8822   ,  0.51221  ,\n",
       "        0.40204  ,  0.21169  , -0.013109 , -0.71616  ,  0.55387  ,\n",
       "        1.1452   , -0.88044  , -0.50216  , -0.22814  ,  0.023885 ,\n",
       "        0.1072   ,  0.083739 ,  0.55015  ,  0.58479  ,  0.75816  ,\n",
       "        0.45706  , -0.28001  ,  0.25225  ,  0.68965  , -0.60972  ,\n",
       "        0.19578  ,  0.044209 , -0.31136  , -0.68826  , -0.22721  ,\n",
       "        0.46185  , -0.77162  ,  0.10208  ,  0.55636  ,  0.067417 ,\n",
       "       -0.57207  ,  0.23735  ,  0.4717   ,  0.82765  , -0.29263  ,\n",
       "       -1.3422   , -0.099277 ,  0.28139  ,  0.41604  ,  0.10583  ,\n",
       "        0.62203  ,  0.89496  , -0.23446  ,  0.51349  ,  0.99379  ,\n",
       "        1.1846   , -0.16364  ,  0.20653  ,  0.73854  ,  0.24059  ,\n",
       "       -0.96473  ,  0.13481  , -0.0072484,  0.33016  , -0.12365  ,\n",
       "        0.27191  , -0.40951  ,  0.021909 , -0.6069   ,  0.40755  ,\n",
       "        0.19566  , -0.41802  ,  0.18636  , -0.032652 , -0.78571  ,\n",
       "       -0.13847  ,  0.044007 , -0.084423 ,  0.04911  ,  0.24104  ,\n",
       "        0.45273  , -0.18682  ,  0.46182  ,  0.089068 , -0.18185  ,\n",
       "       -0.01523  , -0.7368   , -0.14532  ,  0.15104  , -0.71493  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.get_vector(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function:\n",
    "- that takes two words as input, $w_1, w_2$ along with an integer $k$ and plots for both words their $k$ most similar words according to some model $m$ (model $m$ in our case is *glove* or *word2vec*).\n",
    "- use a 2 dimensional [UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) to plot the embeddings\n",
    "- color intensity should be proportional to the vector-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.8099379539489746),\n",
       " ('dog', 0.760945737361908),\n",
       " ('kitten', 0.7464984655380249)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.similar_by_word(\"cat\", topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word analogy\n",
    "\n",
    "*Kitten is to cat as puppy to dog* can be formally written as:\n",
    "*kitten : cat :: puppy : dog*\n",
    "\n",
    "If we had found such a vector representation, we could do something like:\n",
    "$$v_{\\text{cat}} + v_{\\text{puppy}} - v_{\\text{kitten}} \\approx v_{\\text{dog}}$$\n",
    "\n",
    "Does this hold for the following analogies?\n",
    "\n",
    "- *kitten : cat :: puppy : dog*\n",
    "- *king : man :: queen : woman*\n",
    "- *Bern : Switzerland :: Rome : Italy*\n",
    "- *yacht : water :: plane : air*\n",
    "\n",
    "Have a look at this [question](https://stackoverflow.com/questions/54580260/understanding-gensim-word2vecs-most-similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Makemore\n",
    "Watch the second part of Part I of [makemore](https://youtu.be/PaCmpygFfXo?si=7PonsCOBoNCcmWHo), from 1:02:56 to the end, and code along. We will come back to this video series in later weeks.\n",
    "\n",
    "To do:\n",
    "- use softmax to calculate the probabilities instead of `counts / counts.sum(1, keepdims=True)`. Check that these two approaches are equivalent using `torch.allclose`.\n",
    "- try to put everything in nice functions (e.g. forward pass, backward pass, calculating losses and update the weights)\n",
    "- what happens if you use different learning rates? Write a function that helps you to find the best learning rate (this is a very important hyperparamter of each NN) and generate a plot similar to this:\n",
    "\n",
    "![learning rate](learning_rate.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See notebook *part_I.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis using Word Embeddings \n",
    "\n",
    "Get sentiment data from [kaggle](https://www.kaggle.com/datasets/tarkkaanko/amazon)\n",
    "\n",
    "Our goal is to train a binary classifier, that is able to predict the label (positive/negative in the data). 0 is negative, 1 is positive.\n",
    "\n",
    "Your task:\n",
    "1. model 1: as a baseline use *opinion_lexicon* from *nltk* and count positive/negative words to classify\n",
    "2. model 2: think about a way how to use word embeddings to transform text (of arbitrary length) into a vector representation (e.g. through some aggregation function like sum, mean, max, etc.)\n",
    "3. model 3: maybe even better, combine the previous two approaches (i.e. from 1. you get positive negative \n",
    "counts and from 2. you get a vector representation, now you concatenate the two)\n",
    "\n",
    "Train a [elastic net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV) and find \"optimal\" hyperparameters of the model ($l_1\\text{-ratio}$ and $\\alpha$) through cross validation.\n",
    "\n",
    "Useful links:\n",
    "- [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "- [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>\"Western Union\" is something of a forgotten cl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>This movie is an incredible piece of work. It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>My wife and I watched this movie because we pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>When I first watched Flatliners, I was amazed....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>Why would this film be so good, but only gross...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I grew up (b. 1965) watching and loving the Th...      0\n",
       "1      When I put this movie in my DVD player, and sa...      0\n",
       "2      Why do people who do not know what a particula...      0\n",
       "3      Even though I have great interest in Biblical ...      0\n",
       "4      Im a die hard Dads Army fan and nothing will e...      1\n",
       "...                                                  ...    ...\n",
       "39995  \"Western Union\" is something of a forgotten cl...      1\n",
       "39996  This movie is an incredible piece of work. It ...      1\n",
       "39997  My wife and I watched this movie because we pl...      0\n",
       "39998  When I first watched Flatliners, I was amazed....      1\n",
       "39999  Why would this film be so good, but only gross...      1\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"movie.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/stephan/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('opinion_lexicon')\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "positive_set = set(opinion_lexicon.positive())\n",
    "negative_set = set(opinion_lexicon.negative())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
