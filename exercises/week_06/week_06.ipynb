{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 06\n",
    "\n",
    "Exercises:\n",
    "1. watch the first part of the video [Makemore part III](https://www.youtube.com/watch?v=P6sfmUTpUmc) (until 1:18:34) and answer the questions:\n",
    "    - Why is initialization important? How can we initialize the weights of a neural network in educated way?\n",
    "    - what is meant with \"saturated tanh\"? Can this also happen with ReLU or sigmoid?\n",
    "    - explain batch normalization in your own words and explain how it can be implemented in a neural network (during training and testing).\n",
    "2. read chapter [1.3 Example: BERT](https://arxiv.org/abs/2501.09223) and answer these questions:\n",
    "    - Is BERT an encoder or decoder? Explain.\n",
    "    - What parts does the loss function consist of?\n",
    "    - What is the difference between the masked language model and the next sentence prediction task?\n",
    "    - explain the training process of BERT in your own words\n",
    "    - what is the [CLS] token used for and how is it used in the training process?\n",
    "    - what is positional encoding and why is it used in BERT?\n",
    "3. RNN with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Batch-Norm and initialization\n",
    "**Q**: Why is initialization important? How can we initialize the weights of a neural network in educated way?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q**: what is meant with \"saturated tanh\"? Can this also happen with ReLU or sigmoid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: explain batch normalization in your own words and explain how it can be implemented in a neural network (during training and testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Is BERT an encoder or decoder? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What parts does the loss function consist of?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What is the difference between the masked language model and the next sentence prediction task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: explain the training process of BERT in your own words. What masking strategy is used and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: what is the [CLS] token used for and how is it used in the training process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: what is positional encoding and why is it used in BERT?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3. RNN with attention (not graded)\n",
    "\n",
    "This exercise is rather large. If you like, you can play around. Techniques of gradient clipping are important for the exam:\n",
    "\n",
    "One approach to deal with this issue, is to clip the gradient, i.e. if it is larger than some value, we clip it\n",
    "- watch this in [video](https://www.youtube.com/watch?v=KrQp1TxTCUY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i grew up (b. 1965) watching and loving the th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when i put this movie in my dvd player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>even though i have great interest in biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im a die hard dads army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  i grew up (b. 1965) watching and loving the th...      0\n",
       "1  when i put this movie in my dvd player, and sa...      0\n",
       "2  why do people who do not know what a particula...      0\n",
       "3  even though i have great interest in biblical ...      0\n",
       "4  im a die hard dads army fan and nothing will e...      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/movie.csv\")\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare text embeddings\n",
    "\n",
    "Goal: generate batches of text embeddings for training a simple RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(text_list, model):\n",
    "    # collect embeddings for each word that is also in embedding model\n",
    "    res = []\n",
    "    for word in text_list:\n",
    "        if model.has_index_for(word):\n",
    "            res.append(model.get_vector(word))\n",
    "    x = np.array(res)\n",
    "\n",
    "    return torch.Tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First text:\n",
      " ['i', 'grew', 'up', '(b.', '1965)', 'watching', 'and', 'loving', 'the', 'thunderbirds.', 'all', 'my', 'mates', 'at', 'school', 'watched.', 'we', 'played', '\"thunderbirds\"', 'before', 'school,', 'during', 'lunch', 'and', 'after', 'school.', 'we', 'all', 'wanted', 'to', 'be', 'virgil', 'or', 'scott.', 'no', 'one', 'wanted', 'to', 'be', 'alan.', 'counting', 'down', 'from', '5', 'became', 'an', 'art', 'form.', 'i', 'took', 'my', 'children', 'to', 'see', 'the', 'movie', 'hoping', 'they', 'would', 'get', 'a', 'glimpse', 'of', 'what', 'i', 'loved', 'as', 'a', 'child.', 'how', 'bitterly', 'disappointing.', 'the', 'only', 'high', 'point', 'was', 'the', 'snappy', 'theme', 'tune.', 'not', 'that', 'it', 'could', 'compare', 'with', 'the', 'original', 'score', 'of', 'the', 'thunderbirds.', 'thankfully', 'early', 'saturday', 'mornings', 'one', 'television', 'channel', 'still', 'plays', 'reruns', 'of', 'the', 'series', 'gerry', 'anderson', 'and', 'his', 'wife', 'created.', 'jonatha', 'frakes', 'should', 'hand', 'in', 'his', 'directors', 'chair,', 'his', 'version', 'was', 'completely', 'hopeless.', 'a', 'waste', 'of', 'film.', 'utter', 'rubbish.', 'a', 'cgi', 'remake', 'may', 'be', 'acceptable', 'but', 'replacing', 'marionettes', 'with', 'homo', 'sapiens', 'subsp.', 'sapiens', 'was', 'a', 'huge', 'error', 'of', 'judgment.']\n",
      "First text length: 151\n",
      "Embedding shape: torch.Size([130, 100])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "first_text = df[\"text\"].iloc[0].split()\n",
    "print(\"First text:\\n\", first_text)\n",
    "print(\"First text length:\", len(first_text))\n",
    "print(\"Embedding shape:\", get_sentence_embedding(first_text, glove).shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first text has originally 151 tokens, of which we find 107 in the glove vocabulary. Each of these representations has 100 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
       "        [ 0.8328,  0.3953,  0.1417,  ...,  0.0155,  0.7329,  0.2221],\n",
       "        [ 0.2147,  0.4337,  0.3396,  ...,  0.0466,  0.8300,  0.4030],\n",
       "        ...,\n",
       "        [-0.1246,  0.8897, -0.0183,  ..., -0.1471,  0.8906,  0.2021],\n",
       "        [-0.3775, -0.1636,  0.9482,  ..., -0.7871, -0.2698, -0.4416],\n",
       "        [-0.1529, -0.2428,  0.8984,  ..., -0.5910,  1.0039,  0.2066]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_embedding(first_text, glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second text has 263 embedded tokens: this is a problem because we actually need the tensors to have the same size. We can solve this by [padding](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html) the shorter text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second text length: 326\n",
      "Embedding shape: torch.Size([263, 100])\n"
     ]
    }
   ],
   "source": [
    "second_text = df[\"text\"].iloc[1].split()\n",
    "print(\"Second text length:\", len(second_text))\n",
    "print(\"Embedding shape:\", get_sentence_embedding(second_text, glove).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def generate_tensor(text_list, model = glove, do_padding=True):\n",
    "    sequences = []\n",
    "\n",
    "    for text in text_list:\n",
    "        sequence = get_sentence_embedding(text, model)\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    # pad\n",
    "    if do_padding:\n",
    "        sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the difference: when we apply padding, we can collect all data into one tensor. This is impossible without padding because the tensors have different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes with padding: torch.Size([2, 263, 100])\n",
      "shapes with padding: [torch.Size([130, 100]), torch.Size([263, 100])]\n"
     ]
    }
   ],
   "source": [
    "test = [first_text, second_text]\n",
    "\n",
    "yes_pad = generate_tensor(test)\n",
    "print(\"shapes with padding:\", yes_pad.shape)\n",
    "no_pad = generate_tensor(test, do_padding=False)\n",
    "print(\"shapes with padding:\", [x.shape for x in no_pad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify, that the padding works as expected: the first 0 is when the padding starts for the sorter text, this is at position 130 as expected. Since the second text is longer, there is no padding at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 130])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(yes_pad.sum(-1) == 0).nonzero()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset, Dataloader and collate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this into pytorch objects:\n",
    "\n",
    "A Dataset lets us retrieve data from any source that we want: [Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MoviesDataset(Dataset):\n",
    "    def __init__(self, df, text_col=\"text\", target_col=\"label\") -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.text_col = text_col\n",
    "        self.target_col = target_col\n",
    "        self.N = len(self.df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.df[self.text_col].iloc[index], self.df[self.target_col].iloc[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"i can't believe people are looking for a plot in this film. this is laural and hardy. lighten up already. these two were a riot. their comic genius is as funny today as it was 70 years ago. not a filthy word out of either mouth and they were able to keep audiences in stitches. their comedy wasn't sophisticated by any stretch. if a whoopee cushion can't make you grin, there's no reason to watch any of the stuff these guys did. it was a simpler time, and people laughed at stuff that was funny without a plot. i guess it takes a simple mind to enjoy this stuff, so i qualify. two man comedy teams don't compute, we're just too sophisticated... aren't we fortunate?\",\n",
       " 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = MoviesDataset(df)\n",
    "\n",
    "# we can now index our class using [index] notation\n",
    "tt[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 2) (8000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(df, test_size=0.2, random_state=54321)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create datasets for train and test. Recall that here we still have dataframes and not tensors (yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_train = MoviesDataset(X_train)\n",
    "movie_test = MoviesDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(movie_train) = 32000\n",
      "len(movie_test) = 8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"<br /><br />the first thing i have to say is that i own jake speed. i've seen it at least 10 times. this movie is one of the most fun movies ever made. the film begins with margaret (karen kopins) trying to find her sister. her sister was kidnapped in paris and the family has heard nothing. along comes jake speed (wayne crawford), telling her exactly where her sister is and making an offer to find her. jake speed is a hero. he doesn't work for money because he just wants to help and have a good adventure. his partner (dennis christopher) follows him around and writes their adventures into novels. this film is a great adventure. it's hilarious, it's action-packed, it's just great. i guess it's a cult film with a very small cult following. crawford is perfect as jake speed and throws out some one-liners that you'll never forget. kopins and christopher are also good as the girl and the sidekick, respectively. john hurt, the guy who's stomach blew up in alien, plays the devilish, pervertish villian which just adds to the fun. in many ways, this film is similar to indiana jones, in some ways it's similar to james bond films. maybe it should have been called indiana bond but whatever it's title is, it's a very enjoyable film.\",\n",
       " 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{len(movie_train) = }\")\n",
    "print(f\"{len(movie_test) = }\")\n",
    "next(iter(movie_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to map words into the embedding space would be to pre-embed everything before hand. But unfortunately this is to memory intensive. An alternative is, to use a [collate-function](https://pytorch.org/docs/stable/data.html#working-with-collate-fn). Using a costum collate-function we can ensure, that all observations in one batch have the same sequence length.\n",
    "\n",
    "The collate-function takes a batch and then applies a transformation to it (in our case we have to ensure that the sequence lengths are equal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, model = glove, device=device):\n",
    "\n",
    "    text = []\n",
    "    labels = []\n",
    "\n",
    "    for t, l in batch:\n",
    "        text.append(t.split(\" \"))\n",
    "        labels.append(l)\n",
    "        \n",
    "    return generate_tensor(text_list=text, model=model).to(device=device), torch.tensor(labels, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important:\n",
    " - batch size is also a hyperparameter\n",
    " - often it is a good strategy to chose large batch sizes (as long as it fits on the GPU)\n",
    " - large batch sizes often lead to more stable gradient updates (think about the extreme case where your batch has only one sample => large variation of gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(movie_train, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(movie_test, shuffle=False, batch_size=batch_size, collate_fn=collate_fn) # shuffling makes no difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: len(train_dataloader) = 250,      \n",
      "samples in total = 32000\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of batches: {len(train_dataloader) = },\\\n",
    "      \\nsamples in total = {len(train_dataloader)*batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader.dataset.N = 32000\n",
      "movie_train.df.shape = (32000, 2)\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "print(f\"{train_dataloader.dataset.N = }\")\n",
    "print(f\"{movie_train.df.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over batches, see here for an example: the collate function turn the words in the dataset into a dense vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = torch.Size([128, 800, 100]), x.device = device(type='cuda', index=0)\n",
      "y.shape = torch.Size([128]), y.device = device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    x = batch[0]\n",
    "    y = batch[1]\n",
    "    break\n",
    "\n",
    "print(f\"{x.shape = }, {x.device = }\")\n",
    "print(f\"{y.shape = }, {y.device = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([800, 100]),\n",
       " tensor([[-0.5706,  0.4418,  0.7010,  ..., -0.6610,  0.4720,  0.3725],\n",
       "         [-0.5426,  0.4148,  1.0322,  ..., -1.2969,  0.7622,  0.4635],\n",
       "         [-0.2709,  0.0440, -0.0203,  ..., -0.4923,  0.6369,  0.2364],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first example in the first batch:\n",
    "x[0,:,:].shape, x[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 1, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the labels\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN model using the last hidden state\n",
    "\n",
    "Let's start with a model:\n",
    " - your class here must have at least a forward pass\n",
    " - other functions are here to make everything self contained, s.t. we can easily play around with different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, rnn, fc_layers, learning_rate, device=device) -> None:\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # layers\n",
    "        self.rnn = rnn\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "\n",
    "        # optimizer: usually this is also a hyperparameter\n",
    "        # especially the learning rate is very important\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss for binary classification\n",
    "\n",
    "        print(f\"model has {self.count_parameters()} parameters.\")\n",
    "\n",
    "    def count_parameters(self):\n",
    "            return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get last hidden state\n",
    "        _, hn = self.rnn(x)\n",
    "        return self.fc_layers(hn)\n",
    "    \n",
    "\n",
    "    def train_one_batch(self, inputs, labels):\n",
    "        labels = labels.to(torch.float)\n",
    "\n",
    "        # Zero your gradients for every batch\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # predict and compute loss\n",
    "        outputs = self(inputs)\n",
    "        outputs = outputs.squeeze().to(torch.float)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "    \n",
    "        self.optimizer.step()\n",
    "\n",
    "        # batch accuracy and loss\n",
    "        label_hat = torch.round(torch.sigmoid(outputs))\n",
    "        acc = (label_hat == labels).sum().item() / len(labels)\n",
    "\n",
    "        return loss.detach(), acc\n",
    "\n",
    "    def train_epochs(self, nb_epochs, training_loader, test_loader = None, verbosity_level=None):\n",
    "\n",
    "        for epoch_index in range(nb_epochs):\n",
    "            print(\"---------------------------------------------------------------\")\n",
    "            print(f\"training epoch {epoch_index}\")\n",
    "    \n",
    "            # put model in training mode\n",
    "            self.train()\n",
    "\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(training_loader):\n",
    "                loss, acc = self.train_one_batch(inputs, labels)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                accuracies.append(acc)\n",
    "\n",
    "                if verbosity_level is not None and (i % verbosity_level) == 0:\n",
    "                    print(f\"  - batch {i = }: loss = {loss:.2f} \\t accuracy = {acc:.2f}\")\n",
    "\n",
    "            print(f\" - epoch loss = {np.mean(losses):.2f} \\t accuracy = {np.mean(accuracies):.2f}\")\n",
    "            if test_loader is not None:\n",
    "                self.prediction(test_loader)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "    def prediction(self, test_loader):\n",
    "        self.eval() # put model in eval mode\n",
    "\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                labels = labels.to(torch.float)\n",
    "\n",
    "                outputs = self(inputs)\n",
    "                outputs = outputs.squeeze().to(torch.float)\n",
    "                test_loss = self.criterion(outputs, labels)\n",
    "\n",
    "                label_hat = torch.round(torch.sigmoid(outputs))\n",
    "                test_acc = (label_hat == labels).sum().item() / len(labels)\n",
    "\n",
    "                test_losses.append(test_loss.item())\n",
    "                test_accuracies.append(test_acc)\n",
    "\n",
    "        print(f\" - test loss = {np.mean(test_losses):.2f} \\t accuracy = {np.mean(test_accuracies):.2f}\")\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 42441 parameters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyRNN(\n",
       "  (rnn): RNN(100, 128, batch_first=True)\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=100, bias=True)\n",
       "    (1): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128 # a hyperparamter\n",
    "learning_rate = 1e-3 # a hyperparamter\n",
    "drop_out = 0 # drop out is a regularization technique, and also a hyperparamter\n",
    "num_layers = 1 # we can stack layers on top of each other if we like\n",
    "output_dim = 1 # we are doing a binary classification\n",
    "\n",
    "rnn = nn.RNN(input_size = embedding_dim,\n",
    "                 hidden_size = hidden_dim,\n",
    "                 num_layers=num_layers,\n",
    "                 batch_first = True,\n",
    "                 dropout = drop_out)\n",
    "        \n",
    "# here we use two fully connected layers that map from hidden_dim (128)->100->1 \n",
    "fc_layers = [nn.Linear(hidden_dim, 100), nn.Linear(100, output_dim)]\n",
    "\n",
    "model = MyRNN(rnn = rnn, fc_layers=fc_layers, learning_rate=learning_rate)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.69 \t accuracy = 0.52\n",
      "  - batch i = 50: loss = 0.69 \t accuracy = 0.55\n",
      "  - batch i = 100: loss = 0.69 \t accuracy = 0.48\n",
      "  - batch i = 150: loss = 0.69 \t accuracy = 0.50\n",
      "  - batch i = 200: loss = 0.70 \t accuracy = 0.42\n",
      " - epoch loss = 0.69 \t accuracy = 0.50\n",
      " - test loss = 0.69 \t accuracy = 0.50\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 1\n",
      "  - batch i = 0: loss = 0.69 \t accuracy = 0.48\n",
      "  - batch i = 50: loss = 0.69 \t accuracy = 0.52\n",
      "  - batch i = 100: loss = 0.69 \t accuracy = 0.53\n",
      "  - batch i = 150: loss = 0.69 \t accuracy = 0.58\n",
      "  - batch i = 200: loss = 0.69 \t accuracy = 0.52\n",
      " - epoch loss = 0.69 \t accuracy = 0.50\n",
      " - test loss = 0.69 \t accuracy = 0.51\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train_epochs(2, training_loader=train_dataloader, test_loader=test_dataloader, verbosity_level=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "- it seems that we are not learning anything. Can you find out why? Tipp: we are always retrieving the last hidden state, but here padding might be a problem. Can you think of a solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to solve the problem yourself. You have to adapt our collate function. The solution is below.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: \n",
    "- Change the collate-function such that it returns for each batch the tripple: (batch-embedding-vectors, batch-labels, sequence-token-length)\n",
    "- The above version of our collate-function returns the tuple (batch-embedding-vectors, batch-labels)\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we saw that the slicing in the return call of the forward pass resulted in zeros: except for sequences in a batch that have as much tokens as the max-sequence (the longest sequence in a batch)\n",
    "- theses zero-vectors are the last hidden states that we are feeding into the FNN and $\\frac{1}{1 + \\exp(0)} = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tensor(text_list, model = glove, do_padding=True):\n",
    "    sequences = []\n",
    "\n",
    "    for text in text_list:\n",
    "        sequence = get_sentence_embedding(text, model)\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    # additionally we add the lengths of the sequences to get the last hidden state\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    \n",
    "    # pad\n",
    "    if do_padding:\n",
    "        sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    return sequences, lengths\n",
    "\n",
    "\n",
    "def collate_fn(batch, model = glove, device=device):\n",
    "    \"\"\" returns now also length of each sequence. With this information we can easily slice at the correct position, where\n",
    "     the true last hidden state is in the RNN for a given sequence. \"\"\"\n",
    "\n",
    "    text = []\n",
    "    labels = []\n",
    "\n",
    "    for t, l in batch:\n",
    "        text.append(t.split(\" \"))\n",
    "        labels.append(l)\n",
    "    \n",
    "    embeddings, lengths = generate_tensor(text_list=text, model=model)\n",
    "    \n",
    "    return embeddings.to(device=device), torch.tensor(labels, device=device), torch.tensor(lengths, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(movie_train, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(movie_test, shuffle=False, batch_size=batch_size, collate_fn=collate_fn) # shuffling makes no difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the forwad function has to be changed slightly:\n",
    "- previously it took simply the batch of embedding-features\n",
    "- now it takes also the sequence length of each sample in the batch, s.t. we can extract the last hidden embedding from the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: \n",
    " - we now have to retrieve the last hidden unit for each sequence in our batch\n",
    " - e.g. one sequence has token length of 11 and another of 123: for the first one we need to extract the hidden unit at position 11 and for the later at 123\n",
    " - write such a function $f$ that takes a batch of hidden units and a vector of sample lengths (as returned by the new collate function) and then returns for each sample the correct hidden unit\n",
    " - next update the forward pass in our MyRNN-class:\n",
    "    - the forward pass gets now additionally the lengths vector\n",
    "    - you push the features through the RNN (same as before)\n",
    "    - then you apply $f$ to gather the correct hidden units and send those through the fully-connected layers `fc_layers`\n",
    "\n",
    "The following functions are helpful: `torch.arange` and [`torch.gather`](https://pytorch.org/docs/stable/generated/torch.gather.html)\n",
    "\n",
    " \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see for a dummy RNN what the return values are and how we can filter the correct ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=10, hidden_size=7, batch_first=True, )\n",
    "\n",
    "# let's create a dummy batch of data: 2 observations, where each of them has 5 timesteps. 10 is the feature dimension\n",
    "input = torch.randn(2, 5, 10)\n",
    "\n",
    "# put it through the rnn and collect both: final hidden state and intermediate ones\n",
    "output, hn = rnn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 7]), torch.Size([1, 2, 7]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: combine them s.t. we have a resulting tensor of shape (2, 6, 20): 2=batch size, 6=all hidden units, 20=dimension of hidden units\n",
    "output.shape, hn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precisely you can see that the hidden states $h_n$ are the latest hidden states and already part of `output`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8713, -0.3952, -0.3394,  0.0560,  0.3431,  0.1890,  0.5058],\n",
       "         [ 0.7042, -0.9685,  0.6398, -0.7054, -0.1472, -0.1512,  0.8225],\n",
       "         [ 0.8866,  0.1049, -0.6180, -0.7283,  0.2903, -0.9167,  0.5151],\n",
       "         [ 0.8283,  0.7434, -0.2989, -0.9555, -0.3861, -0.8627,  0.4620],\n",
       "         [ 0.9597,  0.0974, -0.6873,  0.4536, -0.7505,  0.4791,  0.1225]],\n",
       "\n",
       "        [[-0.5444, -0.3370, -0.6820,  0.1762, -0.0344, -0.3766, -0.2422],\n",
       "         [ 0.4514, -0.8578,  0.1957, -0.5108,  0.7105,  0.3281, -0.0221],\n",
       "         [ 0.0843,  0.2750, -0.9111, -0.2197, -0.7814, -0.1471, -0.5853],\n",
       "         [ 0.3917,  0.3306, -0.3886, -0.9585, -0.7365,  0.6093, -0.5776],\n",
       "         [ 0.8507, -0.8646, -0.4349,  0.7716, -0.9789,  0.1340, -0.2637]]],\n",
       "       grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9597,  0.0974, -0.6873,  0.4536, -0.7505,  0.4791,  0.1225],\n",
       "         [ 0.8507, -0.8646, -0.4349,  0.7716, -0.9789,  0.1340, -0.2637]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 7])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.squeeze(0).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 7]),\n",
       " tensor([[[ 0.8713, -0.3952, -0.3394,  0.0560,  0.3431,  0.1890,  0.5058],\n",
       "          [ 0.7042, -0.9685,  0.6398, -0.7054, -0.1472, -0.1512,  0.8225],\n",
       "          [ 0.8866,  0.1049, -0.6180, -0.7283,  0.2903, -0.9167,  0.5151],\n",
       "          [ 0.8283,  0.7434, -0.2989, -0.9555, -0.3861, -0.8627,  0.4620],\n",
       "          [ 0.9597,  0.0974, -0.6873,  0.4536, -0.7505,  0.4791,  0.1225],\n",
       "          [ 0.9597,  0.0974, -0.6873,  0.4536, -0.7505,  0.4791,  0.1225]],\n",
       " \n",
       "         [[-0.5444, -0.3370, -0.6820,  0.1762, -0.0344, -0.3766, -0.2422],\n",
       "          [ 0.4514, -0.8578,  0.1957, -0.5108,  0.7105,  0.3281, -0.0221],\n",
       "          [ 0.0843,  0.2750, -0.9111, -0.2197, -0.7814, -0.1471, -0.5853],\n",
       "          [ 0.3917,  0.3306, -0.3886, -0.9585, -0.7365,  0.6093, -0.5776],\n",
       "          [ 0.8507, -0.8646, -0.4349,  0.7716, -0.9789,  0.1340, -0.2637],\n",
       "          [ 0.8507, -0.8646, -0.4349,  0.7716, -0.9789,  0.1340, -0.2637]]],\n",
       "        grad_fn=<CatBackward0>))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = torch.cat((output, hn.squeeze(0).unsqueeze(1)), dim=1)\n",
    "combined.shape, combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the last two rows in each example are identical: the `out` from the RNN contains all hidden states, `h_n` is the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, lengths):\n",
    "    lengths = lengths - 1\n",
    "    out, _ = self.rnn(x)\n",
    "        \n",
    "    # Create indices for gathering\n",
    "    indices = torch.arange(out.size(1), device=self.device)  # Assuming the sequence length is along the second dimension\n",
    "\n",
    "    # Expand the indices to match the shape of ind\n",
    "    indices = indices.unsqueeze(0).expand(lengths.size(0), -1).to(device=self.device)\n",
    "\n",
    "    # Gather the slices\n",
    "    gathered_slices = torch.gather(out, 1, indices.unsqueeze(-1).expand(-1, -1, out.size(-1))).to(device=self.device)\n",
    "\n",
    "    # Select the slices based on the provided indices\n",
    "    out = gathered_slices[torch.arange(out.size(0)), lengths]\n",
    "\n",
    "    return self.fc_layers(out)\n",
    "\n",
    "def train_one_batch(self, inputs, labels, lengths):\n",
    "        labels = labels.to(torch.float)\n",
    "\n",
    "        # Zero your gradients for every batch\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # predict and compute loss\n",
    "        outputs = self(inputs, lengths)\n",
    "        outputs = outputs.squeeze().to(torch.float)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "\n",
    "        # batch accuracy and loss\n",
    "        label_hat = torch.round(torch.sigmoid(outputs))\n",
    "        acc = (label_hat == labels).sum().item() / len(labels)\n",
    "\n",
    "        return loss.detach(), acc\n",
    "    \n",
    "\n",
    "def train_epochs(self, nb_epochs, training_loader, test_loader = None, verbosity_level=None):\n",
    "\n",
    "    for epoch_index in range(nb_epochs):\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "        print(f\"training epoch {epoch_index}\")\n",
    "    \n",
    "        # put model in training mode\n",
    "        self.train()\n",
    "\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "\n",
    "        for i, (inputs, labels, lengths) in enumerate(training_loader):\n",
    "            loss, acc = self.train_one_batch(inputs, labels, lengths)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(acc)\n",
    "\n",
    "            if verbosity_level is not None and (i % verbosity_level) == 0:\n",
    "                print(f\"  - batch {i = }: loss = {loss:.2f} \\t accuracy = {acc:.2f}\")\n",
    "\n",
    "        print(f\" - epoch loss = {np.mean(losses):.2f} \\t accuracy = {np.mean(accuracies):.2f}\")\n",
    "\n",
    "        if test_loader is not None:\n",
    "            self.prediction(test_loader)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def prediction(self, test_loader):\n",
    "    self.eval()\n",
    "\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels, lengths) in test_loader:\n",
    "            labels = labels.to(torch.float)\n",
    "\n",
    "            outputs = self(inputs, lengths)\n",
    "            outputs = outputs.squeeze().to(torch.float)\n",
    "            test_loss = self.criterion(outputs, labels)\n",
    "\n",
    "            label_hat = torch.round(torch.sigmoid(outputs))\n",
    "            test_acc = (label_hat == labels).sum().item() / len(labels)\n",
    "\n",
    "            test_losses.append(test_loss.item())\n",
    "            test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\" - test loss = {np.mean(test_losses):.2f} \\t accuracy = {np.mean(test_accuracies):.2f}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward(self, x):\n",
      "        # get last hidden state\n",
      "        _, hn = self.rnn(x)\n",
      "        return self.fc_layers(hn)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(MyRNN.forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self, x, lengths):\n",
      "    lengths = lengths - 1\n",
      "    out, _ = self.rnn(x)\n",
      "        \n",
      "    # Create indices for gathering\n",
      "    indices = torch.arange(out.size(1), device=self.device)  # Assuming the sequence length is along the second dimension\n",
      "\n",
      "    # Expand the indices to match the shape of ind\n",
      "    indices = indices.unsqueeze(0).expand(lengths.size(0), -1).to(device=self.device)\n",
      "\n",
      "    # Gather the slices\n",
      "    gathered_slices = torch.gather(out, 1, indices.unsqueeze(-1).expand(-1, -1, out.size(-1))).to(device=self.device)\n",
      "\n",
      "    # Select the slices based on the provided indices\n",
      "    out = gathered_slices[torch.arange(out.size(0)), lengths]\n",
      "\n",
      "    return self.fc_layers(out)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MyRNN.forward = forward\n",
    "MyRNN.train_one_batch = train_one_batch\n",
    "MyRNN.train_epochs = train_epochs\n",
    "MyRNN.prediction = prediction\n",
    "print(inspect.getsource(MyRNN.forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test again using only one layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 29569 parameters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyRNN(\n",
       "  (rnn): RNN(100, 128, batch_first=True)\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128 # a hyperparamter\n",
    "learning_rate = 1e-3 # a hyperparamter\n",
    "drop_out = 0 # drop out is a regularization technique, and also a hyperparamter\n",
    "num_layers = 1 # we can stack layers on top of each other if we like\n",
    "output_dim = 1 # we are doing a binary classification\n",
    "\n",
    "rnn = nn.RNN(input_size = embedding_dim,\n",
    "                 hidden_size = hidden_dim,\n",
    "                 num_layers=num_layers,\n",
    "                 batch_first = True,\n",
    "                 dropout = drop_out)\n",
    "        \n",
    "# now we try only one layer\n",
    "fc_layers = [nn.Linear(hidden_dim, 1)]\n",
    "\n",
    "model = MyRNN(rnn = rnn, fc_layers=fc_layers, learning_rate=learning_rate)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.71 \t accuracy = 0.43\n",
      "  - batch i = 20: loss = 0.72 \t accuracy = 0.48\n",
      "  - batch i = 40: loss = 0.66 \t accuracy = 0.63\n",
      "  - batch i = 60: loss = 0.67 \t accuracy = 0.58\n",
      "  - batch i = 80: loss = 0.67 \t accuracy = 0.63\n",
      "  - batch i = 100: loss = 0.61 \t accuracy = 0.71\n",
      "  - batch i = 120: loss = 0.68 \t accuracy = 0.60\n",
      "  - batch i = 140: loss = 0.66 \t accuracy = 0.66\n",
      "  - batch i = 160: loss = 0.60 \t accuracy = 0.73\n",
      "  - batch i = 180: loss = 0.64 \t accuracy = 0.66\n",
      "  - batch i = 200: loss = 0.70 \t accuracy = 0.52\n",
      "  - batch i = 220: loss = 0.69 \t accuracy = 0.52\n",
      "  - batch i = 240: loss = 0.68 \t accuracy = 0.53\n",
      " - epoch loss = 0.67 \t accuracy = 0.59\n",
      "---------------------------------------------------------------\n",
      "training epoch 1\n",
      "  - batch i = 0: loss = 0.68 \t accuracy = 0.57\n",
      "  - batch i = 20: loss = 0.67 \t accuracy = 0.62\n",
      "  - batch i = 40: loss = 0.68 \t accuracy = 0.55\n",
      "  - batch i = 60: loss = 0.69 \t accuracy = 0.58\n",
      "  - batch i = 80: loss = 0.67 \t accuracy = 0.59\n",
      "  - batch i = 100: loss = 0.65 \t accuracy = 0.62\n",
      "  - batch i = 120: loss = 0.67 \t accuracy = 0.57\n",
      "  - batch i = 140: loss = 0.63 \t accuracy = 0.67\n",
      "  - batch i = 160: loss = 0.69 \t accuracy = 0.53\n",
      "  - batch i = 180: loss = 0.69 \t accuracy = 0.54\n",
      "  - batch i = 200: loss = 0.69 \t accuracy = 0.55\n",
      "  - batch i = 220: loss = 0.70 \t accuracy = 0.50\n",
      "  - batch i = 240: loss = 0.70 \t accuracy = 0.45\n",
      " - epoch loss = 0.68 \t accuracy = 0.56\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train_epochs(2, training_loader=train_dataloader, verbosity_level=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a strange pattern: the model gets better and better in the first epoch and worse afterwards in the second epoch.\n",
    "\n",
    "Let's look at the gradients:\n",
    "- here we update the `train_one_batch` and the `train_epochs` functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(self, inputs, labels, lengths):\n",
    "        labels = labels.to(torch.float)\n",
    "\n",
    "        # Zero your gradients for every batch\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # predict and compute loss\n",
    "        outputs = self(inputs, lengths)\n",
    "        outputs = outputs.squeeze().to(torch.float)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "\n",
    "        # get gradients\n",
    "        total_norm = 0.\n",
    "        for p in self.parameters():\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "\n",
    "        # batch accuracy and loss\n",
    "        label_hat = torch.round(torch.sigmoid(outputs))\n",
    "        acc = (label_hat == labels).sum().item() / len(labels)\n",
    "\n",
    "        return loss.detach(), acc, total_norm\n",
    "\n",
    "def train_epochs(self, nb_epochs, training_loader, test_loader=None, verbosity_level=None):\n",
    "\n",
    "        for epoch_index in range(nb_epochs):\n",
    "            print(\"---------------------------------------------------------------\")\n",
    "            print(f\"training epoch {epoch_index}\")\n",
    "    \n",
    "            # put model in training mode\n",
    "            self.train()\n",
    "\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "\n",
    "            for i, (inputs, labels, lengths) in enumerate(training_loader):\n",
    "                loss, acc, total_norm = self.train_one_batch(inputs, labels, lengths)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                accuracies.append(acc)\n",
    "\n",
    "                if verbosity_level is not None and (i % verbosity_level) == 0:\n",
    "                    print(f\"  - batch {i = }: loss = {loss:.2f} \\t accuracy = {acc:.2f} \\tgradient_norm = {total_norm:.2f}\")\n",
    "\n",
    "            print(f\" - epoch loss = {np.mean(losses):.2f} \\taccuracy = {np.mean(accuracies):.2f}\")\n",
    "            \n",
    "            if test_loader is not None:\n",
    "                self.prediction(test_loader)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "MyRNN.train_one_batch = train_one_batch\n",
    "MyRNN.train_epochs = train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 29569 parameters.\n",
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.70 \t accuracy = 0.50 \tgradient_norm = 0.36\n",
      "  - batch i = 20: loss = 0.70 \t accuracy = 0.55 \tgradient_norm = 0.57\n",
      "  - batch i = 40: loss = 0.59 \t accuracy = 0.70 \tgradient_norm = 6.89\n",
      "  - batch i = 60: loss = 0.61 \t accuracy = 0.67 \tgradient_norm = 36.73\n",
      "  - batch i = 80: loss = 0.68 \t accuracy = 0.53 \tgradient_norm = 0.34\n",
      "  - batch i = 100: loss = 0.68 \t accuracy = 0.55 \tgradient_norm = 0.26\n",
      "  - batch i = 120: loss = 0.67 \t accuracy = 0.59 \tgradient_norm = 0.33\n",
      "  - batch i = 140: loss = 0.65 \t accuracy = 0.65 \tgradient_norm = 0.24\n",
      "  - batch i = 160: loss = 0.61 \t accuracy = 0.66 \tgradient_norm = 0.49\n",
      "  - batch i = 180: loss = 0.71 \t accuracy = 0.53 \tgradient_norm = 0.63\n",
      "  - batch i = 200: loss = 0.62 \t accuracy = 0.70 \tgradient_norm = 0.32\n",
      "  - batch i = 220: loss = 0.66 \t accuracy = 0.56 \tgradient_norm = 0.78\n",
      "  - batch i = 240: loss = 0.65 \t accuracy = 0.64 \tgradient_norm = 0.53\n",
      " - epoch loss = 0.67 \taccuracy = 0.59\n",
      " - test loss = 0.64 \t accuracy = 0.63\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 1\n",
      "  - batch i = 0: loss = 0.64 \t accuracy = 0.67 \tgradient_norm = 0.45\n",
      "  - batch i = 20: loss = 0.55 \t accuracy = 0.73 \tgradient_norm = 0.90\n",
      "  - batch i = 40: loss = 0.59 \t accuracy = 0.66 \tgradient_norm = 0.33\n",
      "  - batch i = 60: loss = 0.59 \t accuracy = 0.74 \tgradient_norm = 0.77\n",
      "  - batch i = 80: loss = 0.64 \t accuracy = 0.60 \tgradient_norm = 0.47\n",
      "  - batch i = 100: loss = 0.61 \t accuracy = 0.70 \tgradient_norm = 0.64\n",
      "  - batch i = 120: loss = 0.71 \t accuracy = 0.55 \tgradient_norm = 0.79\n",
      "  - batch i = 140: loss = 0.68 \t accuracy = 0.63 \tgradient_norm = 0.30\n",
      "  - batch i = 160: loss = 0.68 \t accuracy = 0.67 \tgradient_norm = 0.17\n",
      "  - batch i = 180: loss = 0.66 \t accuracy = 0.58 \tgradient_norm = 0.16\n",
      "  - batch i = 200: loss = 0.68 \t accuracy = 0.55 \tgradient_norm = 0.31\n",
      "  - batch i = 220: loss = 0.67 \t accuracy = 0.59 \tgradient_norm = 0.26\n",
      "  - batch i = 240: loss = 0.59 \t accuracy = 0.69 \tgradient_norm = 4.51\n",
      " - epoch loss = 0.65 \taccuracy = 0.62\n",
      " - test loss = 0.64 \t accuracy = 0.64\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128 # a hyperparamter\n",
    "learning_rate = 1e-3 # a hyperparamter\n",
    "drop_out = 0 # drop out is a regularization technique, and also a hyperparamter\n",
    "num_layers = 1 # we can stack layers on top of each other if we like\n",
    "output_dim = 1 # we are doing a binary classification\n",
    "\n",
    "rnn = nn.RNN(input_size = embedding_dim,\n",
    "                 hidden_size = hidden_dim,\n",
    "                 num_layers=num_layers,\n",
    "                 batch_first = True,\n",
    "                 dropout = drop_out)\n",
    "        \n",
    "# now we try only one layer\n",
    "fc_layers = [nn.Linear(hidden_dim, 1)]\n",
    "\n",
    "model = MyRNN(rnn = rnn, fc_layers=fc_layers, learning_rate=learning_rate)\n",
    "model.to(device)\n",
    "\n",
    "model.train_epochs(2, training_loader=train_dataloader, test_loader=test_dataloader, verbosity_level=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run it a few times you see this pattern: large gradients and low accuracy go often hand in hand. A sign of exploding gradients.\n",
    "\n",
    "One approach to deal with this issue, is to clip the gradient, i.e. if it is larger than some value, we clip it\n",
    "- watch this in [video](https://www.youtube.com/watch?v=KrQp1TxTCUY)\n",
    "- you don't have to, but if you are interested here is an [article](https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem) that goes into more details\n",
    "\n",
    "As you saw, there are two approaches to address this problem:\n",
    "- gradient clipping by norm\n",
    "- gradient clipping by threshold\n",
    "\n",
    "Update your code and implement gradient clipping by norm\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(self, inputs, labels, lengths, do_norm_clipping=True):\n",
    "    labels = labels.to(torch.float)\n",
    "\n",
    "    # Zero your gradients for every batch\n",
    "    self.optimizer.zero_grad()\n",
    "\n",
    "    # predict and compute loss\n",
    "    outputs = self(inputs, lengths)\n",
    "    outputs = outputs.squeeze().to(torch.float)\n",
    "    loss = self.criterion(outputs, labels)\n",
    "\n",
    "    # compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    if do_norm_clipping:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "    self.optimizer.step()\n",
    "\n",
    "    # get gradients\n",
    "    total_norm = 0.\n",
    "    for p in self.parameters():\n",
    "        param_norm = p.grad.detach().data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "\n",
    "    # batch accuracy and loss\n",
    "    label_hat = torch.round(torch.sigmoid(outputs))\n",
    "    acc = (label_hat == labels).sum().item() / len(labels)\n",
    "\n",
    "    return loss.detach(), acc, total_norm\n",
    "\n",
    "MyRNN.train_one_batch = train_one_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: it looks better (at least for the runs here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 29569 parameters.\n",
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.70 \t accuracy = 0.53 \tgradient_norm = 0.16\n",
      "  - batch i = 20: loss = 0.69 \t accuracy = 0.54 \tgradient_norm = 0.29\n",
      "  - batch i = 40: loss = 0.66 \t accuracy = 0.61 \tgradient_norm = 0.23\n",
      "  - batch i = 60: loss = 0.66 \t accuracy = 0.62 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.57 \t accuracy = 0.71 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.63 \t accuracy = 0.65 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.59 \t accuracy = 0.72 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.60 \t accuracy = 0.69 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.60 \t accuracy = 0.69 \tgradient_norm = 1.00\n",
      "  - batch i = 180: loss = 0.59 \t accuracy = 0.70 \tgradient_norm = 0.91\n",
      "  - batch i = 200: loss = 0.58 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.57 \t accuracy = 0.73 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.50 \t accuracy = 0.77 \tgradient_norm = 1.00\n",
      " - epoch loss = 0.63 \taccuracy = 0.65\n",
      " - test loss = 0.56 \t accuracy = 0.72\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 1\n",
      "  - batch i = 0: loss = 0.65 \t accuracy = 0.61 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.61 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 40: loss = 0.64 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 60: loss = 0.56 \t accuracy = 0.71 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.63 \t accuracy = 0.63 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.58 \t accuracy = 0.71 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.58 \t accuracy = 0.71 \tgradient_norm = 0.91\n",
      "  - batch i = 140: loss = 0.55 \t accuracy = 0.72 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.50 \t accuracy = 0.78 \tgradient_norm = 0.44\n",
      "  - batch i = 180: loss = 0.62 \t accuracy = 0.67 \tgradient_norm = 0.90\n",
      "  - batch i = 200: loss = 0.54 \t accuracy = 0.74 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.60 \t accuracy = 0.68 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.65 \t accuracy = 0.65 \tgradient_norm = 1.00\n",
      " - epoch loss = 0.60 \taccuracy = 0.70\n",
      " - test loss = 0.69 \t accuracy = 0.55\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128 # a hyperparamter\n",
    "learning_rate = 1e-3 # a hyperparamter\n",
    "drop_out = 0 # drop out is a regularization technique, and also a hyperparamter\n",
    "num_layers = 1 # we can stack layers on top of each other if we like\n",
    "output_dim = 1 # we are doing a binary classification\n",
    "\n",
    "rnn = nn.RNN(input_size = embedding_dim,\n",
    "                 hidden_size = hidden_dim,\n",
    "                 num_layers=num_layers,\n",
    "                 batch_first = True,\n",
    "                 dropout = drop_out)\n",
    "        \n",
    "# now we try only one layer\n",
    "fc_layers = [nn.Linear(hidden_dim, 1)]\n",
    "\n",
    "model = MyRNN(rnn = rnn, fc_layers=fc_layers, learning_rate=learning_rate)\n",
    "model.to(device)\n",
    "\n",
    "model.train_epochs(2, training_loader=train_dataloader, test_loader=test_dataloader, verbosity_level=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important issue before we start with different hyperparmeterizations is the weight initialization. Initially the weights are initialized using random numbers. Often it turns out, that the chosen initialization is an important choice.\n",
    "\n",
    "Luckily there are appropriate initializations (&rarr; Deep Learning Courses):\n",
    "- below I copied our latest changes and make weight initialization accessible through the init function\n",
    "- in the init we now do have Xavier initializations, look [here](https://www.deeplearning.ai/ai-notes/initialization/index.html) if you like or you can also use He initialization (video exercise 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, rnn, fc_layers, learning_rate, do_weight_init = True, device=device) -> None:\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # layers\n",
    "        self.rnn = rnn\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "\n",
    "        # optimizer: usually this is also a hyperparameter\n",
    "        # especially the learning rate is very important\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss for binary classification\n",
    "\n",
    "\n",
    "        if do_weight_init:\n",
    "            self._weight_initialization()\n",
    "        \n",
    "        print(f\"model has {self.count_parameters()} parameters.\")\n",
    "\n",
    "    def _weight_initialization(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                init.xavier_uniform_(param)\n",
    "\n",
    "    def count_parameters(self):\n",
    "            return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        lengths = lengths - 1\n",
    "        \n",
    "        out, _ = self.rnn(x)\n",
    "        \n",
    "        # Create indices for gathering\n",
    "        indices = torch.arange(out.size(1), device=self.device)  # Assuming the sequence length is along the second dimension\n",
    "\n",
    "        # Expand the indices to match the shape of ind\n",
    "        indices = indices.unsqueeze(0).expand(lengths.size(0), -1).to(device=self.device)\n",
    "\n",
    "        # Gather the slices\n",
    "        gathered_slices = torch.gather(out, 1, indices.unsqueeze(-1).expand(-1, -1, out.size(-1))).to(device=self.device)\n",
    "\n",
    "        # Select the slices based on the provided indices\n",
    "        out = gathered_slices[torch.arange(out.size(0)), lengths]\n",
    "\n",
    "        return self.fc_layers(out)\n",
    "    \n",
    "    def train_one_batch(self, inputs, labels, lengths, do_norm_clipping=True):\n",
    "        labels = labels.to(torch.float)\n",
    "\n",
    "        # Zero your gradients for every batch\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # predict and compute loss\n",
    "        outputs = self(inputs, lengths)\n",
    "        outputs = outputs.squeeze().to(torch.float)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        if do_norm_clipping:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "\n",
    "        # get gradients\n",
    "        total_norm = 0.\n",
    "        for p in self.parameters():\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "\n",
    "        # batch accuracy and loss\n",
    "        label_hat = torch.round(torch.sigmoid(outputs))\n",
    "        acc = (label_hat == labels).sum().item() / len(labels)\n",
    "\n",
    "        return loss.detach(), acc, total_norm\n",
    "    \n",
    "\n",
    "    def train_epochs(self, nb_epochs, training_loader, test_loader=None, verbosity_level=None):\n",
    "\n",
    "        for epoch_index in range(nb_epochs):\n",
    "            print(\"---------------------------------------------------------------\")\n",
    "            print(f\"training epoch {epoch_index}\")\n",
    "    \n",
    "            # put model in training mode\n",
    "            self.train()\n",
    "\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "\n",
    "            for i, (inputs, labels, lengths) in enumerate(training_loader):\n",
    "                loss, acc, total_norm = self.train_one_batch(inputs, labels, lengths)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                accuracies.append(acc)\n",
    "\n",
    "                if verbosity_level is not None and (i % verbosity_level) == 0:\n",
    "                    print(f\"  - batch {i = }: loss = {loss:.2f} \\t accuracy = {acc:.2f} \\tgradient_norm = {total_norm:.2f}\")\n",
    "\n",
    "            print(f\" - epoch loss = {np.mean(losses):.2f} \\taccuracy = {np.mean(accuracies):.2f}\")\n",
    "            if test_loader is not None:\n",
    "                self.prediction(test_loader)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "    def prediction(self, test_loader):\n",
    "        self.eval()\n",
    "\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (inputs, labels, lengths) in test_loader:\n",
    "                labels = labels.to(torch.float)\n",
    "\n",
    "                outputs = self(inputs, lengths)\n",
    "                outputs = outputs.squeeze().to(torch.float)\n",
    "                test_loss = self.criterion(outputs, labels)\n",
    "\n",
    "                label_hat = torch.round(torch.sigmoid(outputs))\n",
    "                test_acc = (label_hat == labels).sum().item() / len(labels)\n",
    "\n",
    "                test_losses.append(test_loss.item())\n",
    "                test_accuracies.append(test_acc)\n",
    "\n",
    "        print(f\" - test loss = {np.mean(test_losses):.2f} \\t accuracy = {np.mean(test_accuracies):.2f}\")\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 29569 parameters.\n",
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.76 \t accuracy = 0.48 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.72 \t accuracy = 0.50 \tgradient_norm = 0.44\n",
      "  - batch i = 40: loss = 0.68 \t accuracy = 0.59 \tgradient_norm = 0.42\n",
      "  - batch i = 60: loss = 0.67 \t accuracy = 0.59 \tgradient_norm = 0.50\n",
      "  - batch i = 80: loss = 0.68 \t accuracy = 0.56 \tgradient_norm = 0.85\n",
      "  - batch i = 100: loss = 0.68 \t accuracy = 0.63 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.63 \t accuracy = 0.63 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.55 \t accuracy = 0.73 \tgradient_norm = 0.92\n",
      "  - batch i = 160: loss = 0.65 \t accuracy = 0.62 \tgradient_norm = 0.55\n",
      "  - batch i = 180: loss = 0.56 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 200: loss = 0.70 \t accuracy = 0.54 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.60 \t accuracy = 0.72 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.69 \t accuracy = 0.59 \tgradient_norm = 1.00\n",
      " - epoch loss = 0.68 \taccuracy = 0.60\n",
      " - test loss = 0.57 \t accuracy = 0.71\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 1\n",
      "  - batch i = 0: loss = 0.49 \t accuracy = 0.80 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.59 \t accuracy = 0.69 \tgradient_norm = 0.83\n",
      "  - batch i = 40: loss = 0.59 \t accuracy = 0.69 \tgradient_norm = 1.00\n",
      "  - batch i = 60: loss = 0.62 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.61 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.52 \t accuracy = 0.78 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.71 \t accuracy = 0.63 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.69 \t accuracy = 0.61 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.70 \t accuracy = 0.63 \tgradient_norm = 1.00\n",
      "  - batch i = 180: loss = 0.75 \t accuracy = 0.58 \tgradient_norm = 1.00\n",
      "  - batch i = 200: loss = 0.76 \t accuracy = 0.56 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.64 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.61 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      " - epoch loss = 0.62 \taccuracy = 0.67\n",
      " - test loss = 0.56 \t accuracy = 0.72\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128 # a hyperparamter\n",
    "learning_rate = 1e-3 # a hyperparamter\n",
    "drop_out = 0 # drop out is a regularization technique, and also a hyperparamter\n",
    "num_layers = 1 # we can stack layers on top of each other if we like\n",
    "output_dim = 1 # we are doing a binary classification\n",
    "\n",
    "rnn = nn.RNN(input_size = embedding_dim,\n",
    "                 hidden_size = hidden_dim,\n",
    "                 num_layers=num_layers,\n",
    "                 batch_first = True,\n",
    "                 dropout = drop_out)\n",
    "        \n",
    "# now we try only one layer\n",
    "fc_layers = [nn.Linear(hidden_dim, 1)]\n",
    "\n",
    "model = MyRNN(rnn = rnn, fc_layers=fc_layers, learning_rate=learning_rate)\n",
    "model.to(device)\n",
    "\n",
    "model.train_epochs(2, training_loader=train_dataloader, test_loader=test_dataloader, verbosity_level=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't make a huge difference. Next we can apply batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, lengths):\n",
    "        lengths = lengths - 1\n",
    "\n",
    "        # add LayerNorm\n",
    "        x = nn.BatchNorm1d(x.shape[1], device=self.device)(x)\n",
    "        #x = nn.LayerNorm(x.shape[2], device=self.device)(x)\n",
    "        out, _ = self.rnn(x)\n",
    "\n",
    "        # Create indices for gathering\n",
    "        indices = torch.arange(out.size(1), device=self.device)  # Assuming the sequence length is along the second dimension\n",
    "\n",
    "        # Expand the indices to match the shape of ind\n",
    "        indices = indices.unsqueeze(0).expand(lengths.size(0), -1).to(device=self.device)\n",
    "\n",
    "        # Gather the slices\n",
    "        gathered_slices = torch.gather(out, 1, indices.unsqueeze(-1).expand(-1, -1, out.size(-1))).to(device=self.device)\n",
    "\n",
    "        # Select the slices based on the provided indices\n",
    "        out = gathered_slices[torch.arange(out.size(0)), lengths]\n",
    "\n",
    "        # add LayerNorm\n",
    "        #out = nn.LayerNorm(out.shape[1], device=self.device)(out)\n",
    "        out = nn.BatchNorm1d(out.shape[1], device=self.device)(out)\n",
    "        \n",
    "        return self.fc_layers(out)\n",
    "\n",
    "MyRNN.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 29569 parameters.\n",
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.87 \t accuracy = 0.53 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.74 \t accuracy = 0.55 \tgradient_norm = 1.00\n",
      "  - batch i = 40: loss = 0.73 \t accuracy = 0.55 \tgradient_norm = 1.00\n",
      "  - batch i = 60: loss = 0.75 \t accuracy = 0.55 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.71 \t accuracy = 0.50 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.72 \t accuracy = 0.58 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.72 \t accuracy = 0.59 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.63 \t accuracy = 0.69 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.61 \t accuracy = 0.67 \tgradient_norm = 1.00\n",
      "  - batch i = 180: loss = 0.72 \t accuracy = 0.59 \tgradient_norm = 1.00\n",
      "  - batch i = 200: loss = 0.66 \t accuracy = 0.61 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.72 \t accuracy = 0.55 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.65 \t accuracy = 0.65 \tgradient_norm = 1.00\n",
      " - epoch loss = 0.67 \taccuracy = 0.61\n",
      " - test loss = 0.62 \t accuracy = 0.67\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 1\n",
      "  - batch i = 0: loss = 0.58 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.58 \t accuracy = 0.73 \tgradient_norm = 1.00\n",
      "  - batch i = 40: loss = 0.58 \t accuracy = 0.76 \tgradient_norm = 1.00\n",
      "  - batch i = 60: loss = 0.67 \t accuracy = 0.60 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.61 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.64 \t accuracy = 0.60 \tgradient_norm = 0.93\n",
      "  - batch i = 120: loss = 0.58 \t accuracy = 0.71 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.60 \t accuracy = 0.67 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.63 \t accuracy = 0.65 \tgradient_norm = 1.00\n",
      "  - batch i = 180: loss = 0.68 \t accuracy = 0.59 \tgradient_norm = 1.00\n",
      "  - batch i = 200: loss = 0.50 \t accuracy = 0.81 \tgradient_norm = 0.89\n",
      "  - batch i = 220: loss = 0.52 \t accuracy = 0.76 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.56 \t accuracy = 0.67 \tgradient_norm = 0.83\n",
      " - epoch loss = 0.61 \taccuracy = 0.67\n",
      " - test loss = 0.59 \t accuracy = 0.69\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128 # a hyperparamter\n",
    "learning_rate = 1e-3 # a hyperparamter\n",
    "drop_out = 0 # drop out is a regularization technique, and also a hyperparamter\n",
    "num_layers = 1 # we can stack layers on top of each other if we like\n",
    "output_dim = 1 # we are doing a binary classification\n",
    "\n",
    "rnn = nn.RNN(\n",
    "    input_size = embedding_dim,\n",
    "    hidden_size = hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    batch_first = True,\n",
    "    dropout = drop_out)\n",
    "        \n",
    "# now we try only one layer\n",
    "fc_layers = [nn.Linear(hidden_dim, 1)]\n",
    "\n",
    "model = MyRNN(rnn = rnn, fc_layers=fc_layers, learning_rate=learning_rate)\n",
    "model.to(device)\n",
    "\n",
    "model.train_epochs(2, training_loader=train_dataloader, test_loader=test_dataloader, verbosity_level=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is not really better, we can try to include a non linearity before doing fc_layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 29569 parameters.\n",
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.76 \t accuracy = 0.55 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.72 \t accuracy = 0.51 \tgradient_norm = 0.72\n",
      "  - batch i = 40: loss = 0.68 \t accuracy = 0.62 \tgradient_norm = 0.69\n",
      "  - batch i = 60: loss = 0.67 \t accuracy = 0.56 \tgradient_norm = 0.73\n",
      "  - batch i = 80: loss = 0.66 \t accuracy = 0.58 \tgradient_norm = 0.61\n",
      "  - batch i = 100: loss = 0.61 \t accuracy = 0.66 \tgradient_norm = 0.65\n",
      "  - batch i = 120: loss = 0.61 \t accuracy = 0.69 \tgradient_norm = 0.73\n",
      "  - batch i = 140: loss = 0.67 \t accuracy = 0.60 \tgradient_norm = 0.79\n",
      "  - batch i = 160: loss = 0.66 \t accuracy = 0.60 \tgradient_norm = 1.00\n",
      "  - batch i = 180: loss = 0.58 \t accuracy = 0.71 \tgradient_norm = 0.67\n",
      "  - batch i = 200: loss = 0.58 \t accuracy = 0.74 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.60 \t accuracy = 0.64 \tgradient_norm = 0.97\n",
      "  - batch i = 240: loss = 0.67 \t accuracy = 0.62 \tgradient_norm = 0.90\n",
      " - epoch loss = 0.66 \taccuracy = 0.60\n",
      " - test loss = 0.66 \t accuracy = 0.61\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 1\n",
      "  - batch i = 0: loss = 0.62 \t accuracy = 0.64 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.59 \t accuracy = 0.69 \tgradient_norm = 0.70\n",
      "  - batch i = 40: loss = 0.65 \t accuracy = 0.62 \tgradient_norm = 0.61\n",
      "  - batch i = 60: loss = 0.59 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.62 \t accuracy = 0.66 \tgradient_norm = 0.77\n",
      "  - batch i = 100: loss = 0.61 \t accuracy = 0.65 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.57 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.64 \t accuracy = 0.62 \tgradient_norm = 0.82\n",
      "  - batch i = 160: loss = 0.58 \t accuracy = 0.65 \tgradient_norm = 1.00\n",
      "  - batch i = 180: loss = 0.55 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 200: loss = 0.62 \t accuracy = 0.61 \tgradient_norm = 0.86\n",
      "  - batch i = 220: loss = 0.69 \t accuracy = 0.59 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.64 \t accuracy = 0.63 \tgradient_norm = 0.71\n",
      " - epoch loss = 0.60 \taccuracy = 0.67\n",
      " - test loss = 0.60 \t accuracy = 0.68\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128 # a hyperparamter\n",
    "learning_rate = 1e-3 # a hyperparamter\n",
    "drop_out = 0 # drop out is a regularization technique, and also a hyperparamter\n",
    "num_layers = 1 # we can stack layers on top of each other if we like\n",
    "output_dim = 1 # we are doing a binary classification\n",
    "\n",
    "rnn = nn.RNN(input_size = embedding_dim,\n",
    "                 hidden_size = hidden_dim,\n",
    "                 num_layers=num_layers,\n",
    "                 batch_first = True,\n",
    "                 dropout = drop_out)\n",
    "        \n",
    "# now we try only one layer\n",
    "fc_layers = [nn.ReLU(),nn.Linear(hidden_dim, 1)]\n",
    "\n",
    "model = MyRNN(rnn = rnn, fc_layers=fc_layers, learning_rate=learning_rate)\n",
    "model.to(device)\n",
    "\n",
    "model.train_epochs(2, training_loader=train_dataloader, test_loader=test_dataloader, verbosity_level=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not seem to be very helpful. More layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 37761 parameters.\n",
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.78 \t accuracy = 0.45 \tgradient_norm = 0.80\n",
      "  - batch i = 20: loss = 0.71 \t accuracy = 0.53 \tgradient_norm = 0.63\n",
      "  - batch i = 40: loss = 0.67 \t accuracy = 0.58 \tgradient_norm = 0.58\n",
      "  - batch i = 60: loss = 0.68 \t accuracy = 0.55 \tgradient_norm = 0.59\n",
      "  - batch i = 80: loss = 0.68 \t accuracy = 0.57 \tgradient_norm = 0.63\n",
      "  - batch i = 100: loss = 0.67 \t accuracy = 0.61 \tgradient_norm = 0.73\n",
      "  - batch i = 120: loss = 0.67 \t accuracy = 0.62 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.64 \t accuracy = 0.56 \tgradient_norm = 0.71\n",
      "  - batch i = 160: loss = 0.62 \t accuracy = 0.70 \tgradient_norm = 0.49\n",
      "  - batch i = 180: loss = 0.69 \t accuracy = 0.57 \tgradient_norm = 0.62\n",
      "  - batch i = 200: loss = 0.62 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.66 \t accuracy = 0.64 \tgradient_norm = 0.60\n",
      "  - batch i = 240: loss = 0.64 \t accuracy = 0.67 \tgradient_norm = 0.85\n",
      " - epoch loss = 0.66 \taccuracy = 0.60\n",
      " - test loss = 0.63 \t accuracy = 0.64\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 1\n",
      "  - batch i = 0: loss = 0.60 \t accuracy = 0.67 \tgradient_norm = 0.54\n",
      "  - batch i = 20: loss = 0.61 \t accuracy = 0.70 \tgradient_norm = 0.65\n",
      "  - batch i = 40: loss = 0.56 \t accuracy = 0.72 \tgradient_norm = 0.58\n",
      "  - batch i = 60: loss = 0.60 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.64 \t accuracy = 0.66 \tgradient_norm = 0.57\n",
      "  - batch i = 100: loss = 0.62 \t accuracy = 0.67 \tgradient_norm = 0.65\n",
      "  - batch i = 120: loss = 0.60 \t accuracy = 0.61 \tgradient_norm = 0.58\n",
      "  - batch i = 140: loss = 0.60 \t accuracy = 0.66 \tgradient_norm = 0.68\n",
      "  - batch i = 160: loss = 0.60 \t accuracy = 0.70 \tgradient_norm = 0.62\n",
      "  - batch i = 180: loss = 0.69 \t accuracy = 0.59 \tgradient_norm = 0.77\n",
      "  - batch i = 200: loss = 0.61 \t accuracy = 0.69 \tgradient_norm = 0.57\n",
      "  - batch i = 220: loss = 0.59 \t accuracy = 0.67 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.66 \t accuracy = 0.59 \tgradient_norm = 0.85\n",
      " - epoch loss = 0.62 \taccuracy = 0.66\n",
      " - test loss = 0.60 \t accuracy = 0.68\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128 # a hyperparamter\n",
    "learning_rate = 1e-3 # a hyperparamter\n",
    "drop_out = 0 # drop out is a regularization technique, and also a hyperparamter\n",
    "num_layers = 1 # we can stack layers on top of each other if we like\n",
    "output_dim = 1 # we are doing a binary classification\n",
    "\n",
    "rnn = nn.RNN(input_size = embedding_dim,\n",
    "                 hidden_size = hidden_dim,\n",
    "                 num_layers=num_layers,\n",
    "                 batch_first = True,\n",
    "                 dropout = drop_out)\n",
    "        \n",
    "# now we try only one layer: recall that layers have to match in dimension, i.e. hidden_dim -> 64 -> 64 -> 1 (here important 64 -> 64)\n",
    "fc_layers = [nn.ReLU(), nn.Linear(hidden_dim, 64), nn.ReLU(), nn.Linear(64, 1)]\n",
    "\n",
    "model = MyRNN(rnn = rnn, fc_layers=fc_layers, learning_rate=learning_rate)\n",
    "model.to(device)\n",
    "model.train_epochs(2, training_loader=train_dataloader, test_loader=test_dataloader, verbosity_level=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we could train longer:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "rnn = nn.RNN(input_size = embedding_dim,\n",
    "                 hidden_size = hidden_dim,\n",
    "                 num_layers=num_layers,\n",
    "                 batch_first = True,\n",
    "                 dropout = drop_out)\n",
    "        \n",
    "# now we try only one layer\n",
    "fc_layers = [nn.ReLU(), nn.Linear(hidden_dim, 64), nn.ReLU(), nn.Linear(64, 1)]\n",
    "\n",
    "model = MyRNN(rnn = rnn, fc_layers=fc_layers, learning_rate=learning_rate)\n",
    "model.to(device)\n",
    "model.train_epochs(10, training_loader=train_dataloader, test_loader=test_dataloader, verbosity_level=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not really work very well - remember that we achieved a test accuracy of 0.79 using elastic net!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see, if we should change the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 37761 parameters.\n",
      "learning rate: 0.1\n",
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.74 \t accuracy = 0.51 \tgradient_norm = 0.97\n",
      "  - batch i = 50: loss = 0.67 \t accuracy = 0.60 \tgradient_norm = 0.06\n",
      "  - batch i = 100: loss = 0.71 \t accuracy = 0.45 \tgradient_norm = 0.10\n",
      "  - batch i = 150: loss = 0.69 \t accuracy = 0.53 \tgradient_norm = 0.07\n",
      "  - batch i = 200: loss = 0.69 \t accuracy = 0.45 \tgradient_norm = 0.07\n",
      " - epoch loss = 0.76 \taccuracy = 0.50\n",
      " - test loss = 0.69 \t accuracy = 0.50\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "model has 37761 parameters.\n",
      "learning rate: 0.01\n",
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.72 \t accuracy = 0.51 \tgradient_norm = 0.67\n",
      "  - batch i = 50: loss = 0.70 \t accuracy = 0.61 \tgradient_norm = 0.26\n",
      "  - batch i = 100: loss = 0.69 \t accuracy = 0.51 \tgradient_norm = 0.24\n",
      "  - batch i = 150: loss = 0.68 \t accuracy = 0.55 \tgradient_norm = 0.19\n",
      "  - batch i = 200: loss = 0.66 \t accuracy = 0.62 \tgradient_norm = 0.20\n",
      " - epoch loss = 0.68 \taccuracy = 0.56\n",
      " - test loss = 0.68 \t accuracy = 0.56\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "model has 37761 parameters.\n",
      "learning rate: 0.0001\n",
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.75 \t accuracy = 0.44 \tgradient_norm = 0.59\n",
      "  - batch i = 50: loss = 0.69 \t accuracy = 0.52 \tgradient_norm = 0.46\n",
      "  - batch i = 100: loss = 0.70 \t accuracy = 0.52 \tgradient_norm = 0.43\n",
      "  - batch i = 150: loss = 0.71 \t accuracy = 0.50 \tgradient_norm = 0.43\n",
      "  - batch i = 200: loss = 0.69 \t accuracy = 0.53 \tgradient_norm = 0.45\n",
      " - epoch loss = 0.70 \taccuracy = 0.51\n",
      " - test loss = 0.69 \t accuracy = 0.52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in [1e-1, 1e-2, 1e-4]:\n",
    "    model = MyRNN(rnn = rnn, fc_layers=fc_layers, learning_rate=lr)\n",
    "    model.to(device)\n",
    "    print(f\"learning rate: {lr}\")\n",
    "    model.train_epochs(1, training_loader=train_dataloader, test_loader=test_dataloader, verbosity_level=50)\n",
    "    print(\"============================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a huge difference, but maybe we can try using a larger learning rate of 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "Let's give attention a try! Recall that in our current version in the forward pass we are ignoring all but the last hidden states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 6]),\n",
       " tensor([[[ 0.0325,  0.5809,  0.0696,  0.0675,  0.3881,  0.3815],\n",
       "          [ 0.0966, -0.1537,  0.1609,  0.3482, -0.0783, -0.1984],\n",
       "          [-0.0360,  0.6509, -0.4764, -0.2346,  0.0666,  0.1816],\n",
       "          [-0.3213,  0.0098,  0.4404,  0.4634,  0.3899,  0.2281],\n",
       "          [ 0.6624,  0.4431,  0.4900,  0.2121, -0.1384, -0.0854]],\n",
       " \n",
       "         [[ 0.3022, -0.1820,  0.4094,  0.2782,  0.3534,  0.2801],\n",
       "          [ 0.0847, -0.0765, -0.7470, -0.2452, -0.3739, -0.3442],\n",
       "          [-0.0805,  0.2477,  0.2492,  0.3788,  0.5024,  0.2896],\n",
       "          [ 0.6400,  0.2728,  0.3495,  0.3839, -0.3025, -0.4391],\n",
       "          [ 0.8876, -0.1754, -0.4368,  0.5128, -0.5406, -0.7968]]],\n",
       "        grad_fn=<TransposeBackward1>))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a mock up example of our logic at the moment\n",
    "rnn = nn.RNN(input_size=3, hidden_size=6, batch_first=True, )\n",
    "\n",
    "# let's create a dummy batch of data: 2 observations, where each of them has 5\n",
    "input = torch.randn(2, 5, 3)\n",
    "\n",
    "# put it through the rnn and collect both: final hidden state and intermediate ones\n",
    "out, _ = rnn(input)\n",
    "out.shape, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what we actually want, is to calculate retrieve the last hidden state $\\mathbf h_l$ and calculate the dot-product with the previous hidden states $\\mathbf h_i, i\\le l$ before feeding it to the softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape=torch.Size([2, 5, 6]), hn.shape=torch.Size([1, 2, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0325,  0.5809,  0.0696,  0.0675,  0.3881,  0.3815],\n",
       "          [ 0.0966, -0.1537,  0.1609,  0.3482, -0.0783, -0.1984],\n",
       "          [-0.0360,  0.6509, -0.4764, -0.2346,  0.0666,  0.1816],\n",
       "          [-0.3213,  0.0098,  0.4404,  0.4634,  0.3899,  0.2281],\n",
       "          [ 0.6624,  0.4431,  0.4900,  0.2121, -0.1384, -0.0854]],\n",
       " \n",
       "         [[ 0.3022, -0.1820,  0.4094,  0.2782,  0.3534,  0.2801],\n",
       "          [ 0.0847, -0.0765, -0.7470, -0.2452, -0.3739, -0.3442],\n",
       "          [-0.0805,  0.2477,  0.2492,  0.3788,  0.5024,  0.2896],\n",
       "          [ 0.6400,  0.2728,  0.3495,  0.3839, -0.3025, -0.4391],\n",
       "          [ 0.8876, -0.1754, -0.4368,  0.5128, -0.5406, -0.7968]]],\n",
       "        grad_fn=<TransposeBackward1>),\n",
       " tensor([[[ 0.6624,  0.4431,  0.4900,  0.2121, -0.1384, -0.0854],\n",
       "          [ 0.8876, -0.1754, -0.4368,  0.5128, -0.5406, -0.7968]]],\n",
       "        grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's ignore here the lengths variable we have to deal with\n",
    "out, hn = rnn(input)\n",
    "print(f\"{out.shape=}, {hn.shape=}\")\n",
    "out, hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pytorch broadcasting to calculate the dot-product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn.shape=torch.Size([1, 2, 6])\n",
      "hn.shape=torch.Size([2, 1, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6624,  0.4431,  0.4900,  0.2121, -0.1384, -0.0854]],\n",
       " \n",
       "         [[ 0.8876, -0.1754, -0.4368,  0.5128, -0.5406, -0.7968]]],\n",
       "        grad_fn=<UnsqueezeBackward0>),\n",
       " torch.Size([2, 1, 6]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{hn.shape=}\")\n",
    "hn = hn.squeeze(0).unsqueeze(1)\n",
    "print(f\"{hn.shape=}\")\n",
    "hn, hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 5, 6]),\n",
       " tensor([[[[ 0.0215,  0.2574,  0.0341,  0.0143, -0.0537, -0.0326],\n",
       "           [ 0.0640, -0.0681,  0.0789,  0.0739,  0.0108,  0.0169],\n",
       "           [-0.0239,  0.2884, -0.2334, -0.0498, -0.0092, -0.0155],\n",
       "           [-0.2129,  0.0044,  0.2158,  0.0983, -0.0540, -0.0195],\n",
       "           [ 0.4388,  0.1963,  0.2401,  0.0450,  0.0192,  0.0073]],\n",
       " \n",
       "          [[ 0.2002, -0.0806,  0.2006,  0.0590, -0.0489, -0.0239],\n",
       "           [ 0.0561, -0.0339, -0.3660, -0.0520,  0.0517,  0.0294],\n",
       "           [-0.0534,  0.1097,  0.1221,  0.0803, -0.0695, -0.0247],\n",
       "           [ 0.4239,  0.1209,  0.1713,  0.0814,  0.0419,  0.0375],\n",
       "           [ 0.5879, -0.0777, -0.2140,  0.1088,  0.0748,  0.0680]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0288, -0.1019, -0.0304,  0.0346, -0.2098, -0.3040],\n",
       "           [ 0.0858,  0.0270, -0.0703,  0.1786,  0.0423,  0.1581],\n",
       "           [-0.0320, -0.1142,  0.2081, -0.1203, -0.0360, -0.1447],\n",
       "           [-0.2852, -0.0017, -0.1924,  0.2376, -0.2108, -0.1818],\n",
       "           [ 0.5879, -0.0777, -0.2140,  0.1088,  0.0748,  0.0680]],\n",
       " \n",
       "          [[ 0.2682,  0.0319, -0.1788,  0.1427, -0.1911, -0.2231],\n",
       "           [ 0.0752,  0.0134,  0.3263, -0.1258,  0.2022,  0.2743],\n",
       "           [-0.0715, -0.0434, -0.1088,  0.1942, -0.2716, -0.2307],\n",
       "           [ 0.5680, -0.0479, -0.1527,  0.1969,  0.1635,  0.3499],\n",
       "           [ 0.7878,  0.0308,  0.1908,  0.2630,  0.2923,  0.6348]]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointwise = out * hn\n",
    "pointwise.shape, pointwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sum along the rows to get the scalar product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2876,  0.6783,  0.3354,  0.1817, -0.0869, -0.0433],\n",
       "         [ 1.2148,  0.0384, -0.0860,  0.2775,  0.0500,  0.0863]],\n",
       "\n",
       "        [[ 0.3854, -0.2686, -0.2990,  0.4393, -0.3395, -0.4042],\n",
       "         [ 1.6277, -0.0152,  0.0767,  0.6710,  0.1953,  0.8051]]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pointwise.sum(2)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0215,  0.2574,  0.0341,  0.0143, -0.0537, -0.0326],\n",
       "         [ 0.0640, -0.0681,  0.0789,  0.0739,  0.0108,  0.0169],\n",
       "         [-0.0239,  0.2884, -0.2334, -0.0498, -0.0092, -0.0155],\n",
       "         [-0.2129,  0.0044,  0.2158,  0.0983, -0.0540, -0.0195],\n",
       "         [ 0.4388,  0.1963,  0.2401,  0.0450,  0.0192,  0.0073]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " tensor([ 0.2876,  0.6783,  0.3354,  0.1817, -0.0869, -0.0433],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pointwise[0,0,:] # first row\n",
    "x, sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next calcualte the softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2835, 0.6547, 0.6038, 0.4761, 0.4658, 0.4676],\n",
       "         [0.7165, 0.3453, 0.3962, 0.5239, 0.5342, 0.5324]],\n",
       "\n",
       "        [[0.2240, 0.4370, 0.4072, 0.4423, 0.3694, 0.2298],\n",
       "         [0.7760, 0.5630, 0.5928, 0.5577, 0.6306, 0.7702]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.nn.functional.softmax(score, dim=1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data we have also the actual length of the sequence. We only use hidden states up to the length of the sequence - hidden states after the sequence lengths are ignored. The easiest way is to set the weights to 0 (or equivalently set the argument to the softmax to $-\\infty$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2]),\n",
       " tensor([[[ 1,  2,  3],\n",
       "          [ 4,  5,  6],\n",
       "          [ 1,  2,  3],\n",
       "          [ 4,  5,  6]],\n",
       " \n",
       "         [[ 7,  8,  9],\n",
       "          [10, 11, 12],\n",
       "          [ 1,  2,  3],\n",
       "          [ 4,  5,  6]]]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the actual lengths \n",
    "l = torch.tensor([1,2])\n",
    "\n",
    "# some batch that we want to mask\n",
    "X = torch.tensor([[[1, 2, 3], [4, 5, 6], [1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12], [1, 2, 3], [4, 5, 6]]])\n",
    "l, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([1, 4]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[:, None].shape,  torch.arange(X.size(1)).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1],\n",
       "         [2]]),\n",
       " tensor([[0, 1, 2, 3]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[:, None],  torch.arange(X.size(1)).unsqueeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.arange(X.size(1)).unsqueeze(0) >= l[:, None]\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    1,     2,     3],\n",
       "         [-9999, -9999, -9999],\n",
       "         [-9999, -9999, -9999],\n",
       "         [-9999, -9999, -9999]],\n",
       "\n",
       "        [[    7,     8,     9],\n",
       "         [   10,    11,    12],\n",
       "         [-9999, -9999, -9999],\n",
       "         [-9999, -9999, -9999]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[mask] = -9999\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, lengths):\n",
    "    lengths = lengths - 1\n",
    "\n",
    "    # add LayerNorm\n",
    "    x = nn.LayerNorm(x.shape[2], device=self.device)(x)\n",
    "    out, _ = self.rnn(x)        \n",
    "\n",
    "    # get last hidden state:\n",
    "    # Create indices for gathering\n",
    "    indices = torch.arange(out.size(1), device=self.device)  # Assuming the sequence length is along the second dimension\n",
    "\n",
    "    # Expand the indices to match the shape of ind\n",
    "    indices = indices.unsqueeze(0).expand(lengths.size(0), -1).to(device=self.device)\n",
    "\n",
    "    # Gather the slices\n",
    "    gathered_slices = torch.gather(out, 1, indices.unsqueeze(-1).expand(-1, -1, out.size(-1))).to(device=self.device)\n",
    "\n",
    "    # Select the slices based on the provided indices\n",
    "    last_hidden = gathered_slices[torch.arange(out.size(0)), lengths]\n",
    "        \n",
    "    # attention calculations\n",
    "    last_hidden = last_hidden.squeeze(0).unsqueeze(1)\n",
    "    pointwise = out * last_hidden\n",
    "\n",
    "    mask = torch.arange(out.size(1), device=device).unsqueeze(0) >= lengths[:, None]\n",
    "    pointwise[mask] = -9999\n",
    "\n",
    "    score = pointwise.sum(2)\n",
    "    weights = nn.Softmax(dim=1)(score)\n",
    "    res = torch.sum(weights.unsqueeze(-1) * out, 1)\n",
    "\n",
    "        \n",
    "\n",
    "    # add LayerNorm\n",
    "    out = nn.LayerNorm(res.shape[1], device=self.device)(res)\n",
    "\n",
    "    return self.fc_layers(out)\n",
    "\n",
    "MyRNN.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 37761 parameters.\n",
      "---------------------------------------------------------------\n",
      "training epoch 0\n",
      "  - batch i = 0: loss = 0.71 \t accuracy = 0.57 \tgradient_norm = 0.79\n",
      "  - batch i = 20: loss = 0.71 \t accuracy = 0.50 \tgradient_norm = 0.61\n",
      "  - batch i = 40: loss = 0.69 \t accuracy = 0.55 \tgradient_norm = 0.91\n",
      "  - batch i = 60: loss = 0.67 \t accuracy = 0.57 \tgradient_norm = 0.70\n",
      "  - batch i = 80: loss = 0.66 \t accuracy = 0.60 \tgradient_norm = 0.59\n",
      "  - batch i = 100: loss = 0.65 \t accuracy = 0.63 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.61 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.62 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.61 \t accuracy = 0.70 \tgradient_norm = 0.61\n",
      "  - batch i = 180: loss = 0.83 \t accuracy = 0.50 \tgradient_norm = 1.00\n",
      "  - batch i = 200: loss = 0.59 \t accuracy = 0.72 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.53 \t accuracy = 0.75 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.66 \t accuracy = 0.57 \tgradient_norm = 1.00\n",
      " - epoch loss = 0.66 \taccuracy = 0.61\n",
      " - test loss = 0.60 \t accuracy = 0.69\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 1\n",
      "  - batch i = 0: loss = 0.58 \t accuracy = 0.73 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.66 \t accuracy = 0.61 \tgradient_norm = 0.61\n",
      "  - batch i = 40: loss = 0.61 \t accuracy = 0.65 \tgradient_norm = 1.00\n",
      "  - batch i = 60: loss = 0.70 \t accuracy = 0.62 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.57 \t accuracy = 0.77 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.60 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.64 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.64 \t accuracy = 0.64 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.76 \t accuracy = 0.56 \tgradient_norm = 1.00\n",
      "  - batch i = 180: loss = 0.59 \t accuracy = 0.69 \tgradient_norm = 1.00\n",
      "  - batch i = 200: loss = 0.65 \t accuracy = 0.61 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.62 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.56 \t accuracy = 0.71 \tgradient_norm = 1.00\n",
      " - epoch loss = 0.62 \taccuracy = 0.67\n",
      " - test loss = 0.55 \t accuracy = 0.74\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 2\n",
      "  - batch i = 0: loss = 0.51 \t accuracy = 0.76 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.62 \t accuracy = 0.69 \tgradient_norm = 1.00\n",
      "  - batch i = 40: loss = 0.60 \t accuracy = 0.69 \tgradient_norm = 1.00\n",
      "  - batch i = 60: loss = 0.66 \t accuracy = 0.67 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.55 \t accuracy = 0.73 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.70 \t accuracy = 0.49 \tgradient_norm = 0.57\n",
      "  - batch i = 120: loss = 0.74 \t accuracy = 0.52 \tgradient_norm = 0.90\n",
      "  - batch i = 140: loss = 0.65 \t accuracy = 0.66 \tgradient_norm = 0.49\n",
      "  - batch i = 160: loss = 0.63 \t accuracy = 0.62 \tgradient_norm = 0.77\n",
      "  - batch i = 180: loss = 0.71 \t accuracy = 0.48 \tgradient_norm = 0.34\n",
      "  - batch i = 200: loss = 0.69 \t accuracy = 0.50 \tgradient_norm = 0.14\n",
      "  - batch i = 220: loss = 0.65 \t accuracy = 0.66 \tgradient_norm = 0.30\n",
      "  - batch i = 240: loss = 0.64 \t accuracy = 0.63 \tgradient_norm = 0.48\n",
      " - epoch loss = 0.64 \taccuracy = 0.63\n",
      " - test loss = 0.58 \t accuracy = 0.70\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 3\n",
      "  - batch i = 0: loss = 0.64 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.52 \t accuracy = 0.77 \tgradient_norm = 1.00\n",
      "  - batch i = 40: loss = 0.60 \t accuracy = 0.68 \tgradient_norm = 1.00\n",
      "  - batch i = 60: loss = 0.68 \t accuracy = 0.60 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.58 \t accuracy = 0.72 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.73 \t accuracy = 0.51 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.71 \t accuracy = 0.59 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.58 \t accuracy = 0.73 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.57 \t accuracy = 0.72 \tgradient_norm = 1.00\n",
      "  - batch i = 180: loss = 0.58 \t accuracy = 0.71 \tgradient_norm = 1.00\n",
      "  - batch i = 200: loss = 0.57 \t accuracy = 0.75 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.57 \t accuracy = 0.71 \tgradient_norm = 0.41\n",
      "  - batch i = 240: loss = 0.53 \t accuracy = 0.76 \tgradient_norm = 0.86\n",
      " - epoch loss = 0.60 \taccuracy = 0.70\n",
      " - test loss = 0.60 \t accuracy = 0.67\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 4\n",
      "  - batch i = 0: loss = 0.60 \t accuracy = 0.67 \tgradient_norm = 0.71\n",
      "  - batch i = 20: loss = 0.59 \t accuracy = 0.72 \tgradient_norm = 0.66\n",
      "  - batch i = 40: loss = 0.60 \t accuracy = 0.70 \tgradient_norm = 0.65\n",
      "  - batch i = 60: loss = 0.68 \t accuracy = 0.68 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.63 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.61 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.61 \t accuracy = 0.63 \tgradient_norm = 0.73\n",
      "  - batch i = 140: loss = 0.65 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.64 \t accuracy = 0.62 \tgradient_norm = 0.73\n",
      "  - batch i = 180: loss = 0.55 \t accuracy = 0.75 \tgradient_norm = 0.38\n",
      "  - batch i = 200: loss = 0.53 \t accuracy = 0.74 \tgradient_norm = 0.50\n",
      "  - batch i = 220: loss = 0.59 \t accuracy = 0.68 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.64 \t accuracy = 0.60 \tgradient_norm = 0.84\n",
      " - epoch loss = 0.63 \taccuracy = 0.66\n",
      " - test loss = 0.63 \t accuracy = 0.59\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 5\n",
      "  - batch i = 0: loss = 0.61 \t accuracy = 0.55 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.63 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 40: loss = 0.69 \t accuracy = 0.59 \tgradient_norm = 1.00\n",
      "  - batch i = 60: loss = 0.61 \t accuracy = 0.66 \tgradient_norm = 0.57\n",
      "  - batch i = 80: loss = 0.57 \t accuracy = 0.73 \tgradient_norm = 0.63\n",
      "  - batch i = 100: loss = 0.58 \t accuracy = 0.73 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.55 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.55 \t accuracy = 0.77 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.58 \t accuracy = 0.71 \tgradient_norm = 1.00\n",
      "  - batch i = 180: loss = 0.66 \t accuracy = 0.57 \tgradient_norm = 0.41\n",
      "  - batch i = 200: loss = 0.66 \t accuracy = 0.58 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.63 \t accuracy = 0.66 \tgradient_norm = 0.35\n",
      "  - batch i = 240: loss = 0.59 \t accuracy = 0.73 \tgradient_norm = 1.00\n",
      " - epoch loss = 0.62 \taccuracy = 0.67\n",
      " - test loss = 0.64 \t accuracy = 0.63\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 6\n",
      "  - batch i = 0: loss = 0.56 \t accuracy = 0.72 \tgradient_norm = 0.88\n",
      "  - batch i = 20: loss = 0.61 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 40: loss = 0.66 \t accuracy = 0.62 \tgradient_norm = 0.67\n",
      "  - batch i = 60: loss = 0.61 \t accuracy = 0.73 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.62 \t accuracy = 0.64 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.59 \t accuracy = 0.73 \tgradient_norm = 0.77\n",
      "  - batch i = 120: loss = 0.57 \t accuracy = 0.72 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.44 \t accuracy = 0.84 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.68 \t accuracy = 0.61 \tgradient_norm = 1.00\n",
      "  - batch i = 180: loss = 0.64 \t accuracy = 0.63 \tgradient_norm = 1.00\n",
      "  - batch i = 200: loss = 0.70 \t accuracy = 0.57 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.60 \t accuracy = 0.67 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.47 \t accuracy = 0.82 \tgradient_norm = 1.00\n",
      " - epoch loss = 0.60 \taccuracy = 0.69\n",
      " - test loss = 0.56 \t accuracy = 0.72\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 7\n",
      "  - batch i = 0: loss = 0.59 \t accuracy = 0.71 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.57 \t accuracy = 0.70 \tgradient_norm = 0.83\n",
      "  - batch i = 40: loss = 0.67 \t accuracy = 0.66 \tgradient_norm = 1.00\n",
      "  - batch i = 60: loss = 0.74 \t accuracy = 0.59 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.56 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.60 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.60 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 140: loss = 0.51 \t accuracy = 0.78 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.51 \t accuracy = 0.77 \tgradient_norm = 1.00\n",
      "  - batch i = 180: loss = 0.56 \t accuracy = 0.73 \tgradient_norm = 0.69\n",
      "  - batch i = 200: loss = 0.50 \t accuracy = 0.80 \tgradient_norm = 0.67\n",
      "  - batch i = 220: loss = 0.65 \t accuracy = 0.63 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.63 \t accuracy = 0.62 \tgradient_norm = 1.00\n",
      " - epoch loss = 0.59 \taccuracy = 0.70\n",
      " - test loss = 0.59 \t accuracy = 0.72\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 8\n",
      "  - batch i = 0: loss = 0.55 \t accuracy = 0.77 \tgradient_norm = 1.00\n",
      "  - batch i = 20: loss = 0.56 \t accuracy = 0.71 \tgradient_norm = 0.43\n",
      "  - batch i = 40: loss = 0.61 \t accuracy = 0.64 \tgradient_norm = 1.00\n",
      "  - batch i = 60: loss = 0.58 \t accuracy = 0.72 \tgradient_norm = 1.00\n",
      "  - batch i = 80: loss = 0.55 \t accuracy = 0.73 \tgradient_norm = 0.47\n",
      "  - batch i = 100: loss = 0.55 \t accuracy = 0.77 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.60 \t accuracy = 0.71 \tgradient_norm = 0.91\n",
      "  - batch i = 140: loss = 0.59 \t accuracy = 0.73 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.56 \t accuracy = 0.74 \tgradient_norm = 0.80\n",
      "  - batch i = 180: loss = 0.61 \t accuracy = 0.72 \tgradient_norm = 0.95\n",
      "  - batch i = 200: loss = 0.66 \t accuracy = 0.61 \tgradient_norm = 0.87\n",
      "  - batch i = 220: loss = 0.59 \t accuracy = 0.71 \tgradient_norm = 0.46\n",
      "  - batch i = 240: loss = 0.65 \t accuracy = 0.71 \tgradient_norm = 0.89\n",
      " - epoch loss = 0.59 \taccuracy = 0.70\n",
      " - test loss = 0.59 \t accuracy = 0.70\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "training epoch 9\n",
      "  - batch i = 0: loss = 0.56 \t accuracy = 0.71 \tgradient_norm = 0.76\n",
      "  - batch i = 20: loss = 0.60 \t accuracy = 0.70 \tgradient_norm = 0.64\n",
      "  - batch i = 40: loss = 0.56 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 60: loss = 0.59 \t accuracy = 0.73 \tgradient_norm = 0.76\n",
      "  - batch i = 80: loss = 0.57 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 100: loss = 0.58 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 120: loss = 0.54 \t accuracy = 0.73 \tgradient_norm = 0.75\n",
      "  - batch i = 140: loss = 0.48 \t accuracy = 0.77 \tgradient_norm = 1.00\n",
      "  - batch i = 160: loss = 0.58 \t accuracy = 0.69 \tgradient_norm = 0.47\n",
      "  - batch i = 180: loss = 0.56 \t accuracy = 0.68 \tgradient_norm = 1.00\n",
      "  - batch i = 200: loss = 0.50 \t accuracy = 0.80 \tgradient_norm = 1.00\n",
      "  - batch i = 220: loss = 0.60 \t accuracy = 0.70 \tgradient_norm = 1.00\n",
      "  - batch i = 240: loss = 0.54 \t accuracy = 0.77 \tgradient_norm = 1.00\n",
      " - epoch loss = 0.59 \taccuracy = 0.70\n",
      " - test loss = 0.60 \t accuracy = 0.68\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128 # a hyperparamter\n",
    "learning_rate = 1e-3 # a hyperparamter\n",
    "drop_out = 0 # drop out is a regularization technique, and also a hyperparamter\n",
    "num_layers = 1 # we can stack layers on top of each other if we like\n",
    "output_dim = 1 # we are doing a binary classification\n",
    "\n",
    "\n",
    "rnn = nn.RNN(input_size = embedding_dim,\n",
    "                 hidden_size = hidden_dim,\n",
    "                 num_layers=num_layers,\n",
    "                 batch_first = True,\n",
    "                 dropout = drop_out)\n",
    "        \n",
    "# now we try only one layer\n",
    "fc_layers = [nn.ReLU(), nn.Linear(hidden_dim, 64), nn.ReLU(), nn.Linear(64, 1)]\n",
    "\n",
    "model = MyRNN(rnn = rnn, fc_layers=fc_layers, learning_rate=learning_rate)\n",
    "model.to(device)\n",
    "model.train_epochs(10, training_loader=train_dataloader, test_loader=test_dataloader, verbosity_level=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Take home messages:\n",
    "\n",
    "- how a pytorch loop goes: forward, zero_grad, backward, optimizer step, etc.\n",
    "- there are many hyperparameters (which ones again?)\n",
    "- padding and retrieving the correct hidden state is non-trivial\n",
    "- gradient clipping, batch norm (and later layer norm), non-linearities\n",
    "- attention\n",
    "- we use way too many parameters for this problem. Remember that we have achieved a test accuracy of 0.79 using elastic net!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
