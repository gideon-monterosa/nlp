{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 01\n",
    "\n",
    "Content:\n",
    "1. Softmax\n",
    "2. Byte-Pair\n",
    "3. Tokenizer\n",
    "4. NLP-Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Rem: you get bonus for this exercise if you answer at least 3 out of 4 questions.\n",
    "\n",
    "Here we are going to answer the question, why softmax is called \"softmax\" and investigate softmax characteristics. The $i \\text{-th}$ component of the softmax is given by:\n",
    "\n",
    "$$f(x_i) := \\text{softmax}(x_i) = \\frac{\\exp x_i}{ \\sum_{j = 1}^k \\exp x_j}$$\n",
    "\n",
    "\n",
    "And thus the vector is $f(\\mathbf x) = [f(x_1), \\dots, f(x_k)]$ where $\\mathbf x = (x_1, \\dots, x_k)$.\n",
    "\n",
    "\n",
    "1. show that $f(\\mathbf x)$ can be interpreted as a probability. To do so, show that $\\sum_{i=1}^k f(x_i) = 1$ and $f(x) > 0 \\quad \\forall x \\in \\mathbb{R}$\n",
    "2. show that softmax is $C^\\infty$, i.e. that you can calculate the derivative of $f(x_i)$ as often as you want with respect to $x_i$. You can use the fact that $\\exp x$ is smooth (smooth means $C ^ \\infty$)\n",
    "3. show that if $x_i \\lt x_j$ then $f(x_i) \\lt f(x_j)$. Does the converse hold as well?\n",
    "4. what is the limit of $f(x_i)$ for $x_i \\to + \\infty$? Same question for $x \\to - \\infty$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-pair encoding\n",
    "\n",
    "Implement Byte-paid encoding and reproduce the example in chapter 2.5.2 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Byte Pair Encoding\"](byte_pair_encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "Go through the HuggingFace tutorial on [tokenizers](https://huggingface.co/learn/nlp-course/chapter2/4) and answer:\n",
    "\n",
    "- why do we need tokenizers?\n",
    "- what is the difference between a character-based, word-based and a subword-based tokenizer? What are the advantages and disadvantages of each?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline\n",
    "\n",
    "Recall the NLP-Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%| echo: false\n",
    "flowchart TD\n",
    "    A[Data Acquistion] --> B[Preprocessing and Normalization]\n",
    "    B --> C[Modelling]\n",
    "    C --> C\n",
    "    C --> D[Model evaluation]\n",
    "    D --> |more preprocessing needed| B\n",
    "    D --> |more/different data needed| A\n",
    "    D --> E[Added Value]\n",
    "    E --> |reiterate| A\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to do some prepocessing and normalization steps. Your task is to do:\n",
    "\n",
    "\n",
    "\n",
    "1. **Data Collection**: Collect a corpus of text data. It is completely up to you.\n",
    "2. **Data Cleaning**: Clean the collected data by removing any irrelevant information such as HTML tags, URLs, numbers, etc. This step depends in the corpus you chose:\n",
    "    - count the vocabulary size before and after cleaning\n",
    "3. **Tokenization**: Apply a tokenizer (e.g. using https://www.nltk.org/):\n",
    "    - count the vocabulary size before and after tokenization\n",
    "    - how much time is needed per word in average?\n",
    "4. **Stopwords Removal**: Identify and remove stopwords from the tokens. Stopwords are common words that do not contribute much to the meaning of a sentence, such as 'the', 'is', 'in', etc.\n",
    "    - count the vocabulary size before and after stopword removal\n",
    "5. **Stemming and Lemmatization**: Apply stemming and lemmatization techniques to the tokens and observe the differences. Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. Lemmatization, on the other hand, reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis.\n",
    "    - count the vocabulary size for the stemmed and lemmatized vocabulary\n",
    "6. **Compare**: \n",
    "    - create a barplot of the results you have collected above (https://plotly.com/python/bar-charts/)\n",
    "\n",
    "Useful libraries:\n",
    "\n",
    "- NLTK, Spacy\n",
    "- plotly or matplotlib for plotting graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
