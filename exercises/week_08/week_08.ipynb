{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "pl.seed_everything(42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:50:07.467594Z",
     "start_time": "2025-05-25T22:50:07.459640Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 08\n",
    "\n",
    "1. Count number of trainable parameters in $\\text{BERT}_{\\text{LARGE}}$\n",
    "2. Go through this [tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html) until *Set Anomaly Detection*\n",
    "3. Read about [KV Caching](https://neptune.ai/blog/transformers-key-value-caching):\n",
    "    - What is it?\n",
    "    - Why is it useful?\n",
    "    - How does it work?\n",
    "    - Make the code in the tutorial run"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight: 31,254,528\n",
      "embeddings.position_embeddings.weight: 524,288\n",
      "embeddings.token_type_embeddings.weight: 2,048\n",
      "embeddings.LayerNorm.weight: 1,024\n",
      "embeddings.LayerNorm.bias: 1,024\n",
      "encoder.layer.0.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.0.attention.self.query.bias: 1,024\n",
      "encoder.layer.0.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.0.attention.self.key.bias: 1,024\n",
      "encoder.layer.0.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.0.attention.self.value.bias: 1,024\n",
      "encoder.layer.0.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.0.attention.output.dense.bias: 1,024\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.0.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.0.intermediate.dense.bias: 4,096\n",
      "encoder.layer.0.output.dense.weight: 4,194,304\n",
      "encoder.layer.0.output.dense.bias: 1,024\n",
      "encoder.layer.0.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.0.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.1.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.1.attention.self.query.bias: 1,024\n",
      "encoder.layer.1.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.1.attention.self.key.bias: 1,024\n",
      "encoder.layer.1.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.1.attention.self.value.bias: 1,024\n",
      "encoder.layer.1.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.1.attention.output.dense.bias: 1,024\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.1.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.1.intermediate.dense.bias: 4,096\n",
      "encoder.layer.1.output.dense.weight: 4,194,304\n",
      "encoder.layer.1.output.dense.bias: 1,024\n",
      "encoder.layer.1.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.1.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.2.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.2.attention.self.query.bias: 1,024\n",
      "encoder.layer.2.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.2.attention.self.key.bias: 1,024\n",
      "encoder.layer.2.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.2.attention.self.value.bias: 1,024\n",
      "encoder.layer.2.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.2.attention.output.dense.bias: 1,024\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.2.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.2.intermediate.dense.bias: 4,096\n",
      "encoder.layer.2.output.dense.weight: 4,194,304\n",
      "encoder.layer.2.output.dense.bias: 1,024\n",
      "encoder.layer.2.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.2.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.3.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.3.attention.self.query.bias: 1,024\n",
      "encoder.layer.3.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.3.attention.self.key.bias: 1,024\n",
      "encoder.layer.3.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.3.attention.self.value.bias: 1,024\n",
      "encoder.layer.3.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.3.attention.output.dense.bias: 1,024\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.3.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.3.intermediate.dense.bias: 4,096\n",
      "encoder.layer.3.output.dense.weight: 4,194,304\n",
      "encoder.layer.3.output.dense.bias: 1,024\n",
      "encoder.layer.3.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.3.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.4.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.4.attention.self.query.bias: 1,024\n",
      "encoder.layer.4.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.4.attention.self.key.bias: 1,024\n",
      "encoder.layer.4.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.4.attention.self.value.bias: 1,024\n",
      "encoder.layer.4.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.4.attention.output.dense.bias: 1,024\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.4.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.4.intermediate.dense.bias: 4,096\n",
      "encoder.layer.4.output.dense.weight: 4,194,304\n",
      "encoder.layer.4.output.dense.bias: 1,024\n",
      "encoder.layer.4.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.4.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.5.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.5.attention.self.query.bias: 1,024\n",
      "encoder.layer.5.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.5.attention.self.key.bias: 1,024\n",
      "encoder.layer.5.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.5.attention.self.value.bias: 1,024\n",
      "encoder.layer.5.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.5.attention.output.dense.bias: 1,024\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.5.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.5.intermediate.dense.bias: 4,096\n",
      "encoder.layer.5.output.dense.weight: 4,194,304\n",
      "encoder.layer.5.output.dense.bias: 1,024\n",
      "encoder.layer.5.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.5.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.6.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.6.attention.self.query.bias: 1,024\n",
      "encoder.layer.6.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.6.attention.self.key.bias: 1,024\n",
      "encoder.layer.6.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.6.attention.self.value.bias: 1,024\n",
      "encoder.layer.6.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.6.attention.output.dense.bias: 1,024\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.6.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.6.intermediate.dense.bias: 4,096\n",
      "encoder.layer.6.output.dense.weight: 4,194,304\n",
      "encoder.layer.6.output.dense.bias: 1,024\n",
      "encoder.layer.6.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.6.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.7.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.7.attention.self.query.bias: 1,024\n",
      "encoder.layer.7.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.7.attention.self.key.bias: 1,024\n",
      "encoder.layer.7.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.7.attention.self.value.bias: 1,024\n",
      "encoder.layer.7.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.7.attention.output.dense.bias: 1,024\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.7.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.7.intermediate.dense.bias: 4,096\n",
      "encoder.layer.7.output.dense.weight: 4,194,304\n",
      "encoder.layer.7.output.dense.bias: 1,024\n",
      "encoder.layer.7.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.7.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.8.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.8.attention.self.query.bias: 1,024\n",
      "encoder.layer.8.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.8.attention.self.key.bias: 1,024\n",
      "encoder.layer.8.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.8.attention.self.value.bias: 1,024\n",
      "encoder.layer.8.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.8.attention.output.dense.bias: 1,024\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.8.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.8.intermediate.dense.bias: 4,096\n",
      "encoder.layer.8.output.dense.weight: 4,194,304\n",
      "encoder.layer.8.output.dense.bias: 1,024\n",
      "encoder.layer.8.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.8.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.9.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.9.attention.self.query.bias: 1,024\n",
      "encoder.layer.9.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.9.attention.self.key.bias: 1,024\n",
      "encoder.layer.9.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.9.attention.self.value.bias: 1,024\n",
      "encoder.layer.9.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.9.attention.output.dense.bias: 1,024\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.9.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.9.intermediate.dense.bias: 4,096\n",
      "encoder.layer.9.output.dense.weight: 4,194,304\n",
      "encoder.layer.9.output.dense.bias: 1,024\n",
      "encoder.layer.9.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.9.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.10.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.10.attention.self.query.bias: 1,024\n",
      "encoder.layer.10.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.10.attention.self.key.bias: 1,024\n",
      "encoder.layer.10.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.10.attention.self.value.bias: 1,024\n",
      "encoder.layer.10.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.10.attention.output.dense.bias: 1,024\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.10.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.10.intermediate.dense.bias: 4,096\n",
      "encoder.layer.10.output.dense.weight: 4,194,304\n",
      "encoder.layer.10.output.dense.bias: 1,024\n",
      "encoder.layer.10.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.10.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.11.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.11.attention.self.query.bias: 1,024\n",
      "encoder.layer.11.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.11.attention.self.key.bias: 1,024\n",
      "encoder.layer.11.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.11.attention.self.value.bias: 1,024\n",
      "encoder.layer.11.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.11.attention.output.dense.bias: 1,024\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.11.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.11.intermediate.dense.bias: 4,096\n",
      "encoder.layer.11.output.dense.weight: 4,194,304\n",
      "encoder.layer.11.output.dense.bias: 1,024\n",
      "encoder.layer.11.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.11.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.12.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.12.attention.self.query.bias: 1,024\n",
      "encoder.layer.12.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.12.attention.self.key.bias: 1,024\n",
      "encoder.layer.12.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.12.attention.self.value.bias: 1,024\n",
      "encoder.layer.12.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.12.attention.output.dense.bias: 1,024\n",
      "encoder.layer.12.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.12.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.12.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.12.intermediate.dense.bias: 4,096\n",
      "encoder.layer.12.output.dense.weight: 4,194,304\n",
      "encoder.layer.12.output.dense.bias: 1,024\n",
      "encoder.layer.12.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.12.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.13.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.13.attention.self.query.bias: 1,024\n",
      "encoder.layer.13.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.13.attention.self.key.bias: 1,024\n",
      "encoder.layer.13.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.13.attention.self.value.bias: 1,024\n",
      "encoder.layer.13.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.13.attention.output.dense.bias: 1,024\n",
      "encoder.layer.13.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.13.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.13.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.13.intermediate.dense.bias: 4,096\n",
      "encoder.layer.13.output.dense.weight: 4,194,304\n",
      "encoder.layer.13.output.dense.bias: 1,024\n",
      "encoder.layer.13.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.13.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.14.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.14.attention.self.query.bias: 1,024\n",
      "encoder.layer.14.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.14.attention.self.key.bias: 1,024\n",
      "encoder.layer.14.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.14.attention.self.value.bias: 1,024\n",
      "encoder.layer.14.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.14.attention.output.dense.bias: 1,024\n",
      "encoder.layer.14.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.14.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.14.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.14.intermediate.dense.bias: 4,096\n",
      "encoder.layer.14.output.dense.weight: 4,194,304\n",
      "encoder.layer.14.output.dense.bias: 1,024\n",
      "encoder.layer.14.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.14.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.15.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.15.attention.self.query.bias: 1,024\n",
      "encoder.layer.15.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.15.attention.self.key.bias: 1,024\n",
      "encoder.layer.15.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.15.attention.self.value.bias: 1,024\n",
      "encoder.layer.15.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.15.attention.output.dense.bias: 1,024\n",
      "encoder.layer.15.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.15.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.15.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.15.intermediate.dense.bias: 4,096\n",
      "encoder.layer.15.output.dense.weight: 4,194,304\n",
      "encoder.layer.15.output.dense.bias: 1,024\n",
      "encoder.layer.15.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.15.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.16.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.16.attention.self.query.bias: 1,024\n",
      "encoder.layer.16.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.16.attention.self.key.bias: 1,024\n",
      "encoder.layer.16.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.16.attention.self.value.bias: 1,024\n",
      "encoder.layer.16.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.16.attention.output.dense.bias: 1,024\n",
      "encoder.layer.16.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.16.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.16.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.16.intermediate.dense.bias: 4,096\n",
      "encoder.layer.16.output.dense.weight: 4,194,304\n",
      "encoder.layer.16.output.dense.bias: 1,024\n",
      "encoder.layer.16.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.16.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.17.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.17.attention.self.query.bias: 1,024\n",
      "encoder.layer.17.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.17.attention.self.key.bias: 1,024\n",
      "encoder.layer.17.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.17.attention.self.value.bias: 1,024\n",
      "encoder.layer.17.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.17.attention.output.dense.bias: 1,024\n",
      "encoder.layer.17.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.17.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.17.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.17.intermediate.dense.bias: 4,096\n",
      "encoder.layer.17.output.dense.weight: 4,194,304\n",
      "encoder.layer.17.output.dense.bias: 1,024\n",
      "encoder.layer.17.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.17.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.18.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.18.attention.self.query.bias: 1,024\n",
      "encoder.layer.18.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.18.attention.self.key.bias: 1,024\n",
      "encoder.layer.18.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.18.attention.self.value.bias: 1,024\n",
      "encoder.layer.18.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.18.attention.output.dense.bias: 1,024\n",
      "encoder.layer.18.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.18.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.18.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.18.intermediate.dense.bias: 4,096\n",
      "encoder.layer.18.output.dense.weight: 4,194,304\n",
      "encoder.layer.18.output.dense.bias: 1,024\n",
      "encoder.layer.18.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.18.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.19.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.19.attention.self.query.bias: 1,024\n",
      "encoder.layer.19.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.19.attention.self.key.bias: 1,024\n",
      "encoder.layer.19.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.19.attention.self.value.bias: 1,024\n",
      "encoder.layer.19.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.19.attention.output.dense.bias: 1,024\n",
      "encoder.layer.19.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.19.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.19.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.19.intermediate.dense.bias: 4,096\n",
      "encoder.layer.19.output.dense.weight: 4,194,304\n",
      "encoder.layer.19.output.dense.bias: 1,024\n",
      "encoder.layer.19.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.19.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.20.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.20.attention.self.query.bias: 1,024\n",
      "encoder.layer.20.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.20.attention.self.key.bias: 1,024\n",
      "encoder.layer.20.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.20.attention.self.value.bias: 1,024\n",
      "encoder.layer.20.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.20.attention.output.dense.bias: 1,024\n",
      "encoder.layer.20.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.20.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.20.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.20.intermediate.dense.bias: 4,096\n",
      "encoder.layer.20.output.dense.weight: 4,194,304\n",
      "encoder.layer.20.output.dense.bias: 1,024\n",
      "encoder.layer.20.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.20.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.21.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.21.attention.self.query.bias: 1,024\n",
      "encoder.layer.21.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.21.attention.self.key.bias: 1,024\n",
      "encoder.layer.21.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.21.attention.self.value.bias: 1,024\n",
      "encoder.layer.21.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.21.attention.output.dense.bias: 1,024\n",
      "encoder.layer.21.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.21.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.21.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.21.intermediate.dense.bias: 4,096\n",
      "encoder.layer.21.output.dense.weight: 4,194,304\n",
      "encoder.layer.21.output.dense.bias: 1,024\n",
      "encoder.layer.21.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.21.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.22.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.22.attention.self.query.bias: 1,024\n",
      "encoder.layer.22.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.22.attention.self.key.bias: 1,024\n",
      "encoder.layer.22.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.22.attention.self.value.bias: 1,024\n",
      "encoder.layer.22.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.22.attention.output.dense.bias: 1,024\n",
      "encoder.layer.22.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.22.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.22.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.22.intermediate.dense.bias: 4,096\n",
      "encoder.layer.22.output.dense.weight: 4,194,304\n",
      "encoder.layer.22.output.dense.bias: 1,024\n",
      "encoder.layer.22.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.22.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.23.attention.self.query.weight: 1,048,576\n",
      "encoder.layer.23.attention.self.query.bias: 1,024\n",
      "encoder.layer.23.attention.self.key.weight: 1,048,576\n",
      "encoder.layer.23.attention.self.key.bias: 1,024\n",
      "encoder.layer.23.attention.self.value.weight: 1,048,576\n",
      "encoder.layer.23.attention.self.value.bias: 1,024\n",
      "encoder.layer.23.attention.output.dense.weight: 1,048,576\n",
      "encoder.layer.23.attention.output.dense.bias: 1,024\n",
      "encoder.layer.23.attention.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.23.attention.output.LayerNorm.bias: 1,024\n",
      "encoder.layer.23.intermediate.dense.weight: 4,194,304\n",
      "encoder.layer.23.intermediate.dense.bias: 4,096\n",
      "encoder.layer.23.output.dense.weight: 4,194,304\n",
      "encoder.layer.23.output.dense.bias: 1,024\n",
      "encoder.layer.23.output.LayerNorm.weight: 1,024\n",
      "encoder.layer.23.output.LayerNorm.bias: 1,024\n",
      "pooler.dense.weight: 1,048,576\n",
      "pooler.dense.bias: 1,024\n",
      "\n",
      "Gesamtanzahl trainierbarer Parameter: 335,141,888\n",
      "BERT-LARGE hat 335,141,888 trainierbare Parameter\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(\n",
    "    hidden_size=1024,\n",
    "    num_hidden_layers=24,\n",
    "    num_attention_heads=16,\n",
    "    intermediate_size=4096,\n",
    "    vocab_size=30522,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2\n",
    ")\n",
    "\n",
    "model = BertModel(config)\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.requires_grad:\n",
    "            params = parameter.numel()\n",
    "            total_params += params\n",
    "            print(f\"{name}: {params:,}\")\n",
    "    print(f\"\\nGesamtanzahl trainierbarer Parameter: {total_params:,}\")\n",
    "    return total_params\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"BERT-LARGE hat {total_params:,} trainierbare Parameter\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:50:11.534599Z",
     "start_time": "2025-05-25T22:50:07.496546Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "def expand_mask(mask):\n",
    "    if mask.ndim == 3:\n",
    "        mask = mask.unsqueeze(1)\n",
    "    while mask.ndim < 4:\n",
    "        mask = mask.unsqueeze(0)\n",
    "    return mask\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = expand_mask(mask)\n",
    "\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:50:11.546222Z",
     "start_time": "2025-05-25T22:50:11.536881Z"
    }
   },
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:50:11.562595Z",
     "start_time": "2025-05-25T22:50:11.548048Z"
    }
   },
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TransformerPredictor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self._create_model()\n",
    "\n",
    "    def _create_model(self):\n",
    "        self.input_net = nn.Sequential(\n",
    "            nn.Dropout(self.hparams.input_dropout),\n",
    "            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
    "        )\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
    "\n",
    "        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n",
    "                                              input_dim=self.hparams.model_dim,\n",
    "                                              dim_feedforward=2*self.hparams.model_dim,\n",
    "                                              num_heads=self.hparams.num_heads,\n",
    "                                              dropout=self.hparams.dropout)\n",
    "\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
    "            nn.LayerNorm(self.hparams.model_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.hparams.dropout),\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        x = self.output_net(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
    "        return attention_maps\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = CosineWarmupScheduler(optimizer,\n",
    "                                             warmup=self.hparams.warmup,\n",
    "                                             max_iters=self.hparams.max_iters)\n",
    "        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:50:11.576893Z",
     "start_time": "2025-05-25T22:50:11.565408Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ReverseDataset(data.Dataset):\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels\n",
    "\n",
    "class ReversePredictor(TransformerPredictor):\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        inp_data, labels = batch\n",
    "        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n",
    "\n",
    "        preds = self.forward(inp_data, add_positional_encoding=True)\n",
    "        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(f\"{mode}_loss\", loss)\n",
    "        self.log(f\"{mode}_acc\", acc)\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")\n",
    "\n",
    "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
    "    if input_data is not None:\n",
    "        input_data = input_data[idx].detach().cpu().numpy()\n",
    "    else:\n",
    "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
    "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
    "\n",
    "    num_heads = attn_maps[0].shape[0]\n",
    "    num_layers = len(attn_maps)\n",
    "    seq_len = input_data.shape[0]\n",
    "    fig_size = 4 if num_heads == 1 else 3\n",
    "\n",
    "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
    "    if num_layers == 1:\n",
    "        ax = [ax]\n",
    "    if num_heads == 1:\n",
    "        ax = [[a] for a in ax]\n",
    "    for row in range(num_layers):\n",
    "        for column in range(num_heads):\n",
    "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
    "            ax[row][column].set_xticks(list(range(seq_len)))\n",
    "            ax[row][column].set_xticklabels(input_data.tolist())\n",
    "            ax[row][column].set_yticks(list(range(seq_len)))\n",
    "            ax[row][column].set_yticklabels(input_data.tolist())\n",
    "            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n",
    "    _ = fig.subplots_adjust(hspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def train_reverse(**kwargs):\n",
    "    import os\n",
    "\n",
    "    root_dir = \"ReverseTask\"\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=10,\n",
    "                         gradient_clip_val=5,\n",
    "                         enable_model_summary=False,\n",
    "                         logger=False)\n",
    "\n",
    "    pretrained_filename = \"ReverseTask.ckpt\"\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = ReversePredictor.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = ReversePredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n",
    "        _ = trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model, result\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:50:11.596044Z",
     "start_time": "2025-05-25T22:50:11.579082Z"
    }
   },
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0872c896c8d541a39588abef800fabcf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c940f07c5f93486e8c1c46fa5552dfe0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a361ffc2c3f4b98b7cb2e0c89ec308f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65002a2ab43045dc849d3c723d799ce8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5498299e761d400cb45a68d7890006b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddc196d628e1427189ff674afaa476a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1afe26aae89546bfa9162cf754e70467"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d9ef809cac445368c3b581e775d121a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edba85fbbbb24123b2275d31cfd44e3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35a41a4acd134a0b9f5a09ec7d584b62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29f6ec779a1b4cc9adaed2a5589d68e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7409859d51b47c79aa1ebe02d7023de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61210a7154e04d96bb518cedbdd847fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e9693d164be4fb3bfa128404c3f6c73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 100.00%\n",
      "Test accuracy: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 400x400 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader = data.DataLoader(dataset(1000), batch_size=128)\n",
    "test_loader = data.DataLoader(dataset(10000), batch_size=128)\n",
    "\n",
    "reverse_model, reverse_result = train_reverse(\n",
    "    input_dim=train_loader.dataset.num_categories,\n",
    "    model_dim=32,\n",
    "    num_heads=1,\n",
    "    num_classes=train_loader.dataset.num_categories,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    lr=5e-4,\n",
    "    warmup=50\n",
    ")\n",
    "\n",
    "print(f\"Val accuracy: {(100.0 * reverse_result['val_acc']):4.2f}%\")\n",
    "print(f\"Test accuracy: {(100.0 * reverse_result['test_acc']):4.2f}%\")\n",
    "\n",
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=reverse_model.hparams.num_classes).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = reverse_model.get_attention_maps(inp_data)\n",
    "\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:50:58.903027Z",
     "start_time": "2025-05-25T22:50:11.598139Z"
    }
   },
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
